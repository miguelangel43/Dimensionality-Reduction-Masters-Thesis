\chapter{Methodology}

% 1. Classification process

% 2. Datasets

% 2. Correlation of components with original features
%   - Show plots (Slope Chart)
%   - 

% 3. Spearman Rank correlation between techniques


\section{Datasets}
This section introduces the datasets utilized in this master's thesis. The aim is to provide a comprehensive overview of the datasets, including their sources, characteristics, and any preprocessing steps applied.

\subsection{Artificial Dataset}\label{section:artificial-dataset}


\subsection{Our Database of Faces (ORL) Dataset}\label{section:orl-dataset}
This dataset was created at the AT\&T Laboratories in Cambridge, UK, in the context of a face recognition project the laboratory was doing with the Speech, Vision and Robotics Group of the Cambridge University Engineering Department. It contains face images taken between April 1992 and April 1994, 10 images of 40 different subjects, a total of 400 images. It consists of grayscale images of 40 individuals, with 10 images per person. The dataset was collected under controlled conditions, with variations in facial expression, lighting conditions, and facial details.

Each image in the ORL dataset has a resolution of 92 pixels by 112 pixels. The images were captured under different poses, including variations in head rotation and tilt, providing a diverse set of facial orientations. The ORL dataset offers a realistic representation of face images encountered in real-world scenarios, making it a suitable choice for evaluating the performance of face recognition algorithms. The dataset has been extensively used for training and testing various dimensionality reduction techniques.

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{The-ORL-database-for-training-and-testing.png}
    \caption{Our Database of Faces (ORL) Dataset}
    \label{fig:orl_faces}
\end{figure}

\subsection{COIL2000}\label{section:coil-dataset}
This dataset used in the CoIL 2000 Challenge contains information on customers of an insurance company. The data consists of 86 variables and includes product usage data and socio-demographic data. The data was supplied by the Dutch data mining company Sentient Machine Research and is based on a real world business problem. The training set contains over 5000 descriptions of customers, including the information of whether or not they have a caravan insurance policy. A test set contains 4000 customers of whom only the organizers know if they have a caravan insurance policy.

The dataset had class imbalance, as a large majority of customers have zero claims, only 348 customers had a claim. Sklearn's \textit{RandomOverSampler} was used to balance it.
%\cite{van2004bias}

\subsection{FIFA}\label{section:fifa-dataset}
The FIFA dataset encompasses data from various editions of the popular FIFA video game series, which serves as a reliable source of player information. The dataset contains detailed attributes for each player, including playing positions, skill ratings (such as dribbling, shooting, and passing), and performance statistics (such as goals, assists, and appearances).

Furthermore, it includes the estimated market value of each player, which has often been used as the dependent variable in a regression or classification task.

\section{Spearman's Rank Correlation Coefficient} \label{section:spearman}
Spearman's rank correlation coefficient is a nonparametric measure of rank correlation. It measures the statistical dependence between the rankings of two variables. It is defined as the Pearson correlation coefficient between the rank variables. For variables $X$, $Y$ converted to ranks $R(X)$, $R(Y)$. The Spearman rank correlation coefficient is calculated in the following way.
\begin{equation}
    r_s = \rho_{R(X), R(Y)} = \frac{cov(R(X),R(Y))}{\sigma_{R(X)} \sigma_{R(Y)}}
\end{equation}

\section{Interpretation of Components}

The objective at hand is to determine the significance that each dimensionality reduction technique assigns to each of the original features. To accomplish this, we compute the ranking of the absolute correlation between the dimensions generated by the technique and the original features.

Before diving into the step-by-step process, it is essential to define some variables that are present in dimensionality reduction tasks: let the input data $X$ be an $n \times p$ matrix, where each row represents a sample and each column, denoted as $X_1, X_2, ..., X_p$, corresponds to a feature. Furthermore, let $V \in \mathbb{R}^{d \times p}$ be the projection matrix, i.e., the matrix that projects $p$-dimensional data into a $d$-dimensional subspace and let $V_i$ represent the $i$-th column of $V$ sorted in descending order by their significance (e.g., by the magnitude of their eigenvalues or the variability they capture). Finally let $P \in \mathbb{R}^{n \times d}$ be the the projection of the data into the $d$-dimensional space, so that $P = XV$ holds, with columns $P_1, P_2,..., P_d$.

The first step is to calculate the correlation between the input data represented in its original features $X_1, X_2, ..., X_p$ and the data projected onto the reduced $d$-dimensions $P_1, P_2, ..., P_d$. Let $r_i$ be the vector that contains the correlations between the input data $X$ and its projection onto the $i$-th dimension $P_i$.

$$
    r_i =
    \begin{bmatrix}
        corr(P_i, X_1) \\
        corr(P_i, X_2) \\
        ...            \\
        corr(P_i, X_p)
    \end{bmatrix}
$$

Correlation can be negative or positive. However, it is the absolute value what determines the degree of significance of the correlation. Therefore, we take the absolute values $|corr(P_i,X_j)|$. Moreover, the difference between the value of correlations is not uniform among the different dimensionality reduction techniques and it prevents us from doing a fair comparison. To address that, we take the position of the absolute correlation in a list that is sorted in ascending order. Let $index(j)$ be the index such that $\#\{i : x_i \leq x_j\} = \lfloor j \rfloor$ for $i, j \in \{1,2,...,p\}$.

$$
    s_i =
    \begin{bmatrix}
        index(|corr(P_i, X_1)|) \\
        index(|corr(P_i, X_2)|) \\
        ...                     \\
        index(|corr(P_i, X_p)|)
    \end{bmatrix}
$$

For example, suppose we have some input data with 3 features $X_1, X_2$ and $X_3$ and we would like to explain the first reduced dimension $P_1$. Suppose also the correlations $corr(P_1, X_1)$, $corr(P_1, X_2)$ and $corr(P_1, X_3)$ are $-0.9, 0.3$ and $0.7$ respectively. Vector $r_1$ would written as
$$
    r_1 =
    \begin{bmatrix}
        -0.9 \\
        0.3  \\
        0.7
    \end{bmatrix},
$$

and vector $s_1$ as

$$
    s_1 =
    \begin{bmatrix}
        3 \\
        1 \\
        2
    \end{bmatrix}.
$$

Vector $s_i$ contains the information about the significance that the $i$-th component assigns to each of the original features. The position in the vector of the element of $s_i$ with the highest number indicates the feature that $s_i$ gives more importance to. In the example above, $X_1$ is the feature that $s_1$ gives more importance to because the first position of $s_1$ has the highest number, $3$. Subsequently, $X_3$ would be the second most important, and $X_2$ the least important. This allows us to compare and determine the similarity or dissimilarity of two components using the \textit{Spearman's rank correlation coefficient} of the absolute correlations. This coefficient is explained in \ref{section:spearman}. We do so by simply taking the correlation of vectors $s_i$ and $s_j$, there is no need for calculating the ranks, as that was done previously.

Following the previous example, suppose $s_1$ corresponds to the first projection of the reduced space discovered by $PCA$. We will refer to $s_1$ as $s_{PCA,1}$ from now on. Furthermore, let $s_{SLMVP,1} = [ 2, 1, 3 ]$ be the vector that corresponds to the first projection of the reduced space discovered by $SLMVP$. We compute the \textit{Spearman's rank correlation coefficient} by calculating $corr(s_{PCA,1}, s_{SLMVP,1})$ in the following manner.

$$
    corr(s_{PCA,1}, s_{SLMVP,1}) =
    corr \left(
    \begin{bmatrix}
            3 \\
            1 \\
            2
        \end{bmatrix},
    \begin{bmatrix}
            2 \\
            1 \\
            3
        \end{bmatrix}
    \right) = 0.5
$$

This way we can effectively calculate the similarities and dissimilarities of the first component calculated with the most prominent dimensionality reduction techniques and conclude that, for the dataset used, they are similar or dissimilar. Figure \ref{fig:bw-heatmap} shows an example of a heat map with the coefficients.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.6\textwidth]{bw_heatmap.png}
    \caption{Example of Heatmap of Spearman Rank Correlation Coefficients}
    \label{fig:bw-heatmap}
\end{figure}

This can be further extended so that the we can compare all the components calculated with one dimensionality reduction technique with all the components calculated with another dimensionality reduction technique. In order to do that, we calculate a vector $s_1^*$ which corresponds to the average of $s_1, ..., s_p$ weighted by the variability that each component captures. This variability can be thought of as the eigenvalues corresponding the eigenvectors that form the projection matrix $V$. As I do not have access to the eigenvalues for some of the dimensionality reduction techniques, this is calculated proportionally to the variation of each component. E.g., $Var(s_1)/\sum_i Var(s_i)$ would be the weight corresponding to $s_1$.