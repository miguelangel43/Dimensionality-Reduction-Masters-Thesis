
\chapter{Results and Discussion}

Dimensionality reduction techniques prove particularly valuable when confronted with datasets with a high number of features. However, it is very challenging to explain and interpret components that are based on a multitude of features. Here, as in many areas of machine learning, there exists a trade-off between complexity and explainability. We try to navigate this trade-off by grouping features into categories when the datasets permits, as it happens in Section \ref{section:fifa}.

This sections shows the results of the evaluation of the dimensionality reduction techniques based on their performance in a classifying task, and proceeds to visually show and explain the components of the discovered subspaces by the best-performing techniques.

\section{Artificial Dataset}

\subsection{Classification Results}
In this section, the performance of various dimensionality reduction techniques is evaluated and compared on an artificially generated dataset. The aim is to identify the techniques that provide the best results in reducing the dimensionality of the dataset.
Table \ref{tab:artificial-dataset} presents the results obtained from evaluating different dimensionality reduction techniques. Each technique is assessed based on accuracy. The techniques are ranked in descending order of overall performance.

% Classification Table
\begin{table}[!ht]
    \begin{tabular}{llllr}
        \toprule
        {}                                      & Dim. Technique & Dim. Params        & Model         & Best Score \\
        Dimensions                              &                &                    &               &            \\
        \midrule
                                                & LLE            & k=30-reg=0.001     & XGBoost       & 0.90       \\
                                                & KPCA           & Linear             & XGBoost       & 0.63       \\
        \multirow{-3}{*}{1}                     & PCA            &                    & KNN           & 0.57       \\
        \rowcolor{lightgray}                    & SLMVP          & Linear             & LDA           & 0.96       \\
        \rowcolor{lightgray}                    & PCA            &                    & Random Forest & 0.96       \\
        \rowcolor{lightgray}\multirow{-3}{*}{2} & KPCA           & Linear             & KNN           & 0.96       \\
                                                & SLMVP          & Linear             & LDA           & 1.00       \\
                                                & LLE            & k=30-reg=0.001     & LDA           & 1.00       \\
        \multirow{-3}{*}{3}                     & PCA            &                    & LDA           & 1.00       \\
        \rowcolor{lightgray}                    & SLMVP          & Polynomial-Order=5 & LDA           & 1.00       \\
        \rowcolor{lightgray}                    & LLE            & k=30-reg=0.001     & KNN           & 1.00       \\
        \rowcolor{lightgray}\multirow{-3}{*}{4} & LPP            & k=17               & LDA           & 1.00       \\
                                                & SLMVP          & Linear             & LDA           & 1.00       \\
                                                & LLE            & k=30-reg=0.001     & KNN           & 1.00       \\
        \multirow{-3}{*}{5}                     & LPP            & k=17               & Random Forest & 1.00       \\
        \bottomrule
    \end{tabular}
    \caption{Performance Comparison of Dimensionality Reduction Techniques on an artificially generated dataset. This dataset contains 20 centers, 300 features and sample size 1000. (See Annex Table \ref{annex-tab:artificial-dataset}) for full results.}
    \label{tab:artificial-dataset}
\end{table}

% Comparison Line Chart
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\textwidth]{artificial_line_chart.png}
    \caption{This figure shows how the accuracy of the best-performing model for each dimensionality reduction technique, evolves as the number of dimensions is increased.}
    \label{fig:art_line_chart}
\end{figure}

For 2, 3, 4, and 5 dimensions, SLMVP, PCA and KPCA demonstrate the best performance among the tested dimensionality reduction techniques. They achieve an accuracy of 96\% in 2 dimensions and are able to correctly classify all the samples with 3 dimensions or more. Both SLMVP and KPCA achieve these results in their linear configurations, paired with the LDA classifier or K-Nearest Neighbors.
However, LLE emerges as the winner when only 1 dimension is allowed in the classification task. It achieves a 90\% accuracy paired with the XGBoost classifier, much higher than the second-best performing, which is KPCA with a 63\% accuracy.

Overall, SLMVP, KPCA and PCA demonstrate to be the most promising dimensionality reduction technique, especially when the classification task is not restricted to just 1 dimension. However, it is interesting to see how the results would change when applying the dimensionality reduction techniques to a real-world dataset.

\subsection{Explanation}

In this section, the results of applying various dimensionality reduction techniques are presented and compared using visual plots. The objective is to analyze and contrast the performance of these techniques in reducing the dimensionality of the dataset.

Figures \ref{fig:artificial_2d} and \ref{fig:artificial_3d} display a scatter plot of the dataset after applying the techniques. The plot shows the reduced-dimensional representation of the data, where the axes represent the principal components. By examining the scatter plot, we can observe the clustering and distribution of the data points in the reduced space.

% 2D Visualization
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{artificial_2d.png}
    \caption{Visualization of the clusters projected into 2 dimensions that resulted from the application of the different dimensionality reduction techniques on the artificially-generated data.}
    \label{fig:artificial_2d}
\end{figure}

% 3D Visualization
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{artificial_3d.png}
    \caption{Visualization of the clusters projected into 3 dimensions that resulted from the application of the different dimensionality reduction techniques on the artificially-generated data.}
    \label{fig:artificial_3d}
\end{figure}

By examining the scatter plots obtained from the different dimensionality reduction techniques, we can compare their performance in capturing and preserving the structure of the original dataset. The following observations can be made:

\begin{itemize}
    \item SLMVP with a linear kernel, PCA and LPP demonstrate a relatively even distribution of data points in the reduced space, indicating that it effectively captures the overall variance of the dataset. However, it may not be able to capture complex nonlinear relationships.
    \item SLMVP and LOL exhibit a clear separation between different classes in the reduced space, even though this is not clearly visible in the LOL plot, since a few clusters lay very far away from where most of them are located. These are clearly separated although they appear to be overlapping. The notable class separation in the plots corresponding to SLMVP and LOL emphasizes the discriminative power between classes, making it useful for classification tasks.
    \item SLMVP with a radial kernel captures the classes in a curve, which can be clearly seen in its 3-dimensional representation (Figure \ref{fig:artificial_3d}).
\end{itemize}

\subsubsection{Comparing Techniques}

% Heatmap
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\textwidth]{heatmap_artificial.png}
    \caption{Heatmap of Spearman Rank Correlation Coefficients}
    \label{fig:heatmap-artificial}
\end{figure}


The heatmap in Figure \ref{fig:heatmap-artificial} visually represents the similarity and dissimilarity between different dimensionality reduction techniques based on their Spearman rank correlation coefficients. The correlation coefficients are calculated by measuring the absolute correlations between the data in the reduced space from each technique and the original data. Higher correlation coefficients indicate a stronger resemblance between the reduced-dimensional representations obtained using 2 dimensionality reduction techniques.
The color and the color intensity in the heatmap represent the magnitude of the correlation coefficients. Darker red shades indicate higher correlation coefficients, indicating a stronger resemblance between the reduced-dimensional representations. Darker blue shades, on the other hand, suggest weaker correlations. We can make the following observations.

\begin{itemize}
    \item SLMVP is most strongly-correlated with LPP and LOL, although this correlation is only moderate, below 60\%. The correlation with LPP could be due to the fact that both SLMVP and LPP are local and therefore incorporate neighborhood information of the dataset. On the other hand, the correlation with LOL could be due to another property that both SLMVP and LOL posses, the fact that both are supervised and therefore incorporate information about the dependent variable.
    \item The strongest correlation over all is between PCA and LOL with 92\%, and the lowest between LLE and LOL with 10\%.
\end{itemize}

\subsubsection{Multilabel}

% Multilabel
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{2D-multilabel.png}
    \caption{Visualization of the clusters projected into 2 dimensions that resulted from the application of the different dimensionality reduction techniques on artificially-generated multilabel data.}
    \label{fig:2D_multilabel}
\end{figure}

Among the supervised dimensionality reduction techniques evaluated, SLMVP stands out as the sole method capable of effectively handling multilabel datasets. This characteristic holds great significance due to the important role that information about the relationships between multiple classes and variables plays in preserving the intrinsic structure of the data during dimensionality reduction. By leveraging SLMVP's unique ability to capture and retain these intricate connections, it ensures a more accurate and comprehensive representation of the data, thereby enhancing the effectiveness of dimensionality reduction tasks.

Figure \ref{fig:2D_multilabel} shows the reduced-dimensional representation of a 2-label dataset. It is worth noting that LOL is not capable of handling multilabel data and is therefore not on the plot. The figure shows that only SLMVP is capable of clearly separating the datapoints into their different four combinations of classes.

\subsubsection{Choosing Features and Components}

We can leverage the calculated absolute correlations to select the features that should be kept for a posterior machine learning task. Figure \ref{fig:hbar-artificial} shows the 20 features with the highest absolute correlations with the first component, in the case of the first subplot, and the average of the first 5 components, in the case of the second subplot. For the 1 component, the first three features stand out with correlations higher than 50\%. For this artificially-generated data, the correlations follow a smooth curve and it is difficult to determine how many features to reject. Thresholds of $0.2$ and $0.15$ for the first and second subplots respectively are reasonable.

\begin{figure}[!ht]
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{hbar_artificial.png}
        \caption{1 Component}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{hbar_artificial_5dim.png}
        \caption{5 Components}
    \end{subfigure}
    \caption{Original features of the artificially-generated dataset ordered by their correlation to the components obtained with SLMVP with radial kernel and gamma values 0.1.}
    \label{fig:hbar-artificial}
\end{figure}


\section{FIFA Dataset}\label{section:fifa}

\subsection{Classification Results}

We assess and compare different techniques for reducing the dimensionality of the FIFA dataset introduced in section \ref{section:fifa-dataset}. We present the evaluation results in Table \ref{tab:fifa-dataset}, which ranks the techniques based on their accuracy.
SLMVP is the clear winner for this dataset. It achieves a higher accuracy than the other techniques for all the numbers of dimensions tested in its radial kernel configuration.

% Classification Table
\begin{table}[!ht]
    \begin{tabular}{llllr}
        \toprule
        {}                                      & Dim. Technique & Dim. Params        & Model         & Accuracy \\
        Dimensions                              &                &                    &               &          \\
        \midrule
                                                & SLMVP          & Linear             & KNN           & 0.86     \\
                                                & LOL            &                    & Naive Bayes   & 0.85     \\
        \multirow{-3}{*}{1}                     & KPCA           & Linear             & LDA           & 0.83     \\
        \rowcolor{lightgray}                    & SLMVP          & Radial-Gammas=0.1  & XGBoost       & 0.96     \\
        \rowcolor{lightgray}                    & LPP            & k=5                & Random Forest & 0.90     \\
        \rowcolor{lightgray}\multirow{-3}{*}{2} & LOL            &                    & KNN           & 0.89     \\
                                                & SLMVP          & Radial-Gammas=0.1  & XGBoost       & 0.96     \\
                                                & KPCA           & Linear             & SVM           & 0.93     \\
        \multirow{-3}{*}{3}                     & LPP            & k=5                & SVM           & 0.93     \\
        \rowcolor{lightgray}                    & SLMVP          & Radial-Gammas=0.01 & Random Forest & 0.96     \\
        \rowcolor{lightgray}                    & LPP            & k=5                & XGBoost       & 0.94     \\
        \rowcolor{lightgray}\multirow{-3}{*}{4} & KPCA           & Linear             & SVM           & 0.90     \\
                                                & SLMVP          & Radial-Gammas=0.1  & SVM           & 0.95     \\
                                                & LLE            & k=30-reg=0.001     & SVM           & 0.94     \\
        \multirow{-3}{*}{5}                     & LOL            &                    & XGBoost       & 0.91     \\
        \bottomrule
    \end{tabular}
    \caption{Performance Comparison of Dimensionality Reduction Techniques on the FIFA dataset. (See Annex Table \ref{annex-tab:fifa-dataset}) for full results.}
    \label{tab:fifa-dataset}
\end{table}

% Comparison Line Chart
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\textwidth]{fifa_line_chart.png}
    \caption{This figure shows how the accuracy of the best-performing model for each dimensionality reduction technique, evolves as the number of dimensions is increased.}
    \label{fig:fifa_line_chart}
\end{figure}

\subsection{Explanation}

% 2D Visualization
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{fifa_2d.png}
    \caption{Visualization of the clusters projected into 2 dimensions that resulted from the application of the different dimensionality reduction techniques on the FIFA dataset.}
    \label{fig:fifa_2d}
\end{figure}

% 3D Visualization
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{fifa_3d.png}
    \caption{Visualization of the clusters projected into 3 dimensions that resulted from the application of the different dimensionality reduction techniques on the FIFA dataset.}
    \label{fig:fifa_3d}
\end{figure}

By looking at the scatter plots created by the different techniques in figure \ref{fig:fifa_2d} and \ref{fig:fifa_3d}, we can compare how well they perform in capturing and preserving the structure of the original dataset. Here are the key observations we can make:

\begin{itemize}
    \item The results from all the techniques except KPCA with a radial kernel, demonstrate a relatively even distribution of data points in the reduced space, indicating that it effectively captures the overall variance of the dataset.
    \item SLMVP with linear and polynomial kernels, PCA and LPP manage to separate the three different classes (corresponding to attacker, defender and midfielder) in a single component. In the case of SLMVP with a linear kernel and PCA, it is the first component the one that captures the separation, whereas the second component captures the variability of the data points along other features that do not seem to have such a great impact on the dependent variable.
    \item As will be shown later, the axes that capture the separation of the classes show a strong correlation with features related to attacking and defending features. Either a positive correlation with attacking and a negative correlation with defending, or a negative correlation with attacking and a positive correlation with defending.
    \item The dimensionality reduction techniques do a great job clearly separating the classes \textit{attacker} and \textit{defender}. However the class \textit{midfielder} lies between the two other classes, often overlapping with them.
\end{itemize}

\subsubsection{Comparing Techniques}

% Heatmap
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\textwidth]{heatmap_fifa.png}
    \caption{Heatmap of Spearman Rank Correlation Coefficients}
    \label{fig:heatmap-fifa}
\end{figure}

The heatmap in Figure \ref{fig:heatmap-fifa} visually represents the similarity and dissimilarity between different dimensionality reduction techniques based on their Spearman rank correlation coefficients. The correlation coefficients are calculated by measuring the absolute correlations between the data in the reduced space from each technique and the original data. Higher correlation coefficients indicate a stronger resemblance between the reduced-dimensional representations obtained using 2 dimensionality reduction techniques. We can make the following observations:

\begin{itemize}
    \item SLMVP is most strongly-correlated with LPP, KPCA and LOL, although this correlation is only moderate, below 60\%. The correlation with LPP could be due to the fact that both SLMVP and LPP are local and therefore incorporate neighborhood information of the dataset. On the other hand, the correlation with LOL could be due to another property that both SLMVP and LOL posses, the fact that both are supervised and therefore incorporate information about the dependent variable. Finally, SLMVP and KPCA both use radial kernels.
    \item The strongest correlation over all is between PCA and LOL with 94\%, and the lowest between KPCA and PCA with 13\%.
\end{itemize}

\subsubsection{Explaining Components}

As mentioned earlier in this section, dimensionality reduction techniques are the most effective when applied to datasets with a large number of features. However, explaining a dataset with many features is challenging, due to the difficulty of showing high-dimensional data in graphs that humans can read. The FIFA dataset helps us address that challenge. The features provide insights about a soccer player's ability and they can be grouped into 6 categories: defending, attacking, skill, power, movement and mentality. For example, the features \textit{attacking\_crossing}, \textit{attacking\_finishing}, \textit{attacking\_heading\_accuracy}, \textit{attacking\_short\_passing}, and \textit{attacking\_volleys} are grouped as attacking.

As explained in section \ref{exp_comp}, the correlations between the input data represented in its original features and the data projected onto the first reduced dimensions is calculated. For the sake of a good visualization, the correlations concerning the features of each of the aforementioned categories are grouped and averaged. For example, \textit{attacking} in Figure \ref{fig:spider_fifa} shows the average of the correlations of \textit{attacking\_crossing}, \textit{attacking\_finishing}, etc, with the first or second component. This is a way of bringing a muti-dimensional problem to a number of variables that we are more comfortable drawing insights from.

% Spider
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{spider_fifa_2_comp.png}
    \caption{Visualization of the average correlation of the first two components with the features grouped by skill set.}
    \label{fig:spider_fifa}
\end{figure}

By looking at the spider plots created by the different techniques in Figure \ref{fig:spider_fifa}, we can make some key observations and explain the components:

\begin{itemize}
    \item Every dimensionality reduction technique, with the exception of LLE, shows high correlation coefficients for \textit{defending}. This feature is key to separate the classes attackers and defenders, because the first shows a high negative correlation with \textit{defending} and the latter shows a high positive correlation. Intuitively this makes sense, as defenders are better at defending than attackers.
    \item LOL's first two components are very similar in their correlations to the original features.
    \item PCA's and LOL's spider plot concerning the first component (marked in blue in the graph) are very similar. This matches what was shown in the heat map of Figure \ref{fig:heatmap-fifa}, where they shared a 96\% Spearman Rank Correlation Coefficient.
\end{itemize}

% 2D -  Spider
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\textwidth]{slmvp_2d_spider.png}
    \caption{Visualization of the average correlation of the first two components with the features grouped by skill set.}
    \label{fig:2d_spider_fifa}
\end{figure}

Now we select the results of \textit{SLMVP Radial-Gammas=0.01} in order to give detailed explanations of their components. Figure \ref{fig:2d_spider_fifa} shows side-by-side a scatterplot with the data points projected into the first 2 components, and a spider plot that conveys how these two components relate to the original features of the dataset. Figure \ref{fig:hbar-fifa-neg} shows the correlations of the first and second component with the original features.

The following insights are drawn:

\begin{itemize}
    \item The first component is positively correlated with \textit{defending}. It separates the attackers from the defenders and midfielders. Attackers are shown to have less defending skills than the other groups.
    \item The second component is positively correlated with \textit{attacking} and \textit{skill}, and negatively correlated with \textit{defending}. It separates the defenders from the attackers and midfielders.
    \item It is clear that attackers and defenders are separated by their defending and attacking skills. However the separation of midfielders with the other classes is not so straightforward.
    \item The bar plot in Figure \ref{fig:hbar-fifa-neg} reveals that information. Besides being better at defending than attackers, midfielders are better at intercepting and making long passes. They are also better than defenders in their vision, positioning and long shots, whereas defenders are better than midfielders in heading accuracy and aggresiveness.
\end{itemize}

\begin{figure}[!ht]
    \centering
    \begin{subfigure}{.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{hbar_fifa_neg1.png}
        \caption{1st Component}
    \end{subfigure}
    \begin{subfigure}{.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{hbar_fifa_neg2.png}
        \caption{2nd component}
    \end{subfigure}
    \caption{Correlations of the first and second component with the original features.}
    \label{fig:hbar-fifa-neg}
\end{figure}

\subsubsection{Choosing Features and Components}

We can leverage the calculated absolute correlations to select the features that should be kept for a posterior machine learning task. Figure \ref{fig:hbar-fifa} shows the 20 features with the highest absolute correlations with the first component, in the case of the first subplot, and the average of the first 5 components, in the case of the second subplot.
The features related to attacking and defending are the ones with the highest absolute correlations, which matches what was shown in Figure \ref{fig:2d_spider_fifa}. Selecting the features with a higher correlation than $0.2$ is reasonable.

\begin{figure}[!ht]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=.6\linewidth]{hbar_fifa.png}
        \caption{1 component}
    \end{subfigure}%

    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=.6\linewidth]{hbar_fifa_5dim.png}
        \caption{5 components}
    \end{subfigure}
    \caption{Original features of the FIFA dataset ordered by their correlation to the components obtained with SLMVP with radial kernel and gamma values 0.1.}
    \label{fig:hbar-fifa}
\end{figure}


\section{Insurance Company Benchmark (COIL 2000) Dataset}

\subsection{Classification Results}

We conduct an evaluation and comparison of various techniques aimed at reducing the dimensionality of the COIL 2000 dataset introduced in Section \ref{section:coil-dataset}. The evaluation results are presented in Table \ref{tab:coil-dataset}, where the techniques are ranked based on their accuracy. SLMVP is again the clear winner for this dataset. It achieves a higher accuracy than the other techniques for dimensions. It stands out at 3 dimensions, where it peaks at an accuracy of 77.5\%. LOL follows closely in second place for all the tested dimensions except at 30 dimensions, where LLE takes its place.

% Classification Table
\begin{table}[!h]
    \begin{tabular}{llllr}
        \toprule
        {}                                       & Dim. Technique & Dim. Params        & Model         & Accuracy \\
        Dimensions                               &                &                    &               &          \\
        \midrule
                                                 & SLMVP          & Radial-Gammas=0.01 & KNN           & 0.760    \\
                                                 & LOL            &                    & Naive Bayes   & 0.730    \\
        \multirow{-3}{*}{1}                      & KPCA           & Linear             & Naive Bayes   & 0.700    \\
        \rowcolor{lightgray}                     & SLMVP          & Radial-Gammas=0.01 & LDA           & 0.775    \\
        \rowcolor{lightgray}                     & LOL            &                    & Naive Bayes   & 0.735    \\
        \rowcolor{lightgray}\multirow{-3}{*}{3}  & LLE            & k=28-reg=0.001     & Naive Bayes   & 0.715    \\
                                                 & SLMVP          & Linear             & Naive Bayes   & 0.750    \\
                                                 & LOL            &                    & Naive Bayes   & 0.725    \\
        \multirow{-3}{*}{5}                      & KPCA           & Linear             & AdaBoost      & 0.715    \\
        \rowcolor{lightgray}                     & SLMVP          & Radial-Gammas=0.01 & KNN           & 0.740    \\
        \rowcolor{lightgray}                     & LOL            &                    & Random Forest & 0.710    \\
        \rowcolor{lightgray}\multirow{-3}{*}{15} & KPCA           & Radial-Gamma=0.007 & Naive Bayes   & 0.705    \\
                                                 & SLMVP          & Linear             & KNN           & 0.755    \\
                                                 & LLE            & k=28-reg=0.001     & LDA           & 0.740    \\
        \multirow{-3}{*}{30}                     & LPP            & k=11               & KNN           & 0.725    \\
        \bottomrule
    \end{tabular}
    \caption{Performance Comparison of Dimensionality Reduction Techniques on the COIL 2000 dataset. (See Annex Table \ref{annex-tab:coil-dataset}) for full results.}
    \label{tab:coil-dataset}
\end{table}

% Comparison Line Chart
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\textwidth]{coil_line_chart.png}
    \caption{This figure shows how the accuracy of the best-performing model for each dimensionality reduction technique, evolves as the number of dimensions is increased.}
    \label{fig:coil_line_chart}
\end{figure}

\subsection{Explanation}

By looking at the scatter plots created by the different techniques in figures \ref{fig:coil-2d} and \ref{fig:coil-3d}, we can compare how well they perform in capturing and preserving the structure of the original dataset. Here are the key observations we can make:

\begin{itemize}
    \item Unlike with the previous datasets, these classes are more difficult to clearly separate. This is reasonable considering that the maximum accuracy that could be achieved in the classification task was 77.5\%. Nonetheless, SLMVP and LOL do a good job separating those 70-80\% of datapoints along the first two axes.
    \item The results from all the techniques, demonstrate a relatively even distribution of data points in the reduced space, indicating that it effectively captures the overall variance of the dataset.
\end{itemize}

% 2D Visualization
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{coil2000_2d.png.png}
    \caption{Visualization of the clusters projected into 2 dimensions that resulted from the application of the different dimensionality reduction techniques on the COIL 2000 dataset.}
    \label{fig:coil-2d}
\end{figure}

% 3D Visualization
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{coil2000_3d.png.png}
    \caption{Visualization of the clusters projected into 3 dimensions that resulted from the application of the different dimensionality reduction techniques on the COIL 2000 dataset.}
    \label{fig:coil-3d}
\end{figure}

\subsubsection{Comparing Techniques}

The heat map in Figure \ref{fig:heatmap-coil} visually represents the similarity and dissimilarity between different dimensionality reduction techniques based on their Spearman rank correlation coefficients. The correlation coefficients are calculated by measuring the absolute correlations between the data in the reduced space from each technique and the original data. Higher correlation coefficients indicate a stronger resemblance between the reduced-dimensional representations obtained using 2 dimensionality reduction techniques. We can make the following observations:

\begin{itemize}
    \item PCA, KPCA and LOL exhibit very high correlations between one another. KPCA was executed in its linear kernel configuration, which yields results that very closely resemble PCA. Furthermore, the similarity between PCA and LOL was also present in other datasets.
    \item SLMVP manifests a moderate correlation with PCA, KPCA and LOL.
    \item LPP stands out as yielding the most unique result, with very low correlations with other techniques.
\end{itemize}

% Heatmap
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\textwidth]{heatmap_coil.png}
    \caption{Heatmap of Spearman Rank Correlation Coefficients}
    \label{fig:heatmap-coil}
\end{figure}

\subsubsection{Choosing Features and Components}

\begin{figure}[!ht]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=.6\linewidth]{hbar_coil.png}
        \caption{1 component}
    \end{subfigure}

    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=.6\linewidth]{hbar_coil_5dim.png}
        \caption{5 components}
    \end{subfigure}
    \caption{Original features of the COIL 2000 dataset ordered by their correlation to the components obtained with SLMVP with radial kernel and gamma values 0.1.}
    \label{fig:hbar-coil}
\end{figure}

We can leverage the calculated absolute correlations to select the features that should be kept for a posterior machine learning task. Figure \ref{fig:hbar-coil} shows the 15 features with the highest absolute correlations with the first component, in the case of the first subplot, and the average of the first 5 components, in the case of the second subplot.

In the first subplot, the features that show the highest absolute correlation with the component are related to whether the client already had some type of insurance. These are \textit{Contribution car policies}, \textit{Number of car policies}, and \textit{Contribution private third party insurance}. We also observe features related to the education level and income of the client.
The highest correlated features at 5 components do differ. We observe \textit{Rented house} and \textit{Home owners} at the top, as well as other features related to the marital status, or whether the client lives in a household with children.

\section{ORL Dataset}

\subsection{Classification Results}

We conduct an evaluation and comparison of various techniques aimed at reducing the dimensionality of the Our Database of Faces (ORL) dataset introduced in Section \ref{section:orl-dataset}. The evaluation results are presented in Table \ref{tab:orl-dataset}, where the techniques are ranked based on their accuracy.

For this dataset, at least 5 components are necessary for any dimensionality reduction technique machine learning model pair to reach a high accuracy (greater than 90\%). LOL is the best performing technique at 5 dimensions. LOL, KPCA, and PCA obtain very high accuracies at 15 dimensions and reach a 100\% at 30 dimensions. LLE and SLMVP underperform, perhaps due to their locality characteristic. The dataset is composed of images of 40 different faces, with variations in head rotation, tilt and pose, and locality can misguide the model.

% Classification Table
\begin{table}[!ht]
    \begin{tabular}{llllr}
        \toprule
        {}                                       & Dim. Technique & Dim. Params    & Model         & Accuracy \\
        Dimensions                               &                &                &               &          \\
        \midrule
                                                 & LLE            & k=18-reg=0.001 & KNN           & 0.300    \\
                                                 & KPCA           & Linear         & Random Forest & 0.200    \\
        \multirow{-3}{*}{1}                      & PCA            &                & XGBoost       & 0.175    \\
        \rowcolor{lightgray}                     & KPCA           & Linear         & SVM           & 0.750    \\
        \rowcolor{lightgray}                     & LLE            & k=18-reg=0.001 & SVM           & 0.750    \\
        \rowcolor{lightgray}\multirow{-3}{*}{3}  & PCA            &                & SVM           & 0.750    \\
                                                 & LOL            &                & SVM           & 0.925    \\
                                                 & KPCA           & Linear         & SVM           & 0.900    \\
        \multirow{-3}{*}{5}                      & PCA            &                & SVM           & 0.900    \\
        \rowcolor{lightgray}                     & KPCA           & Linear         & SVM           & 0.975    \\
        \rowcolor{lightgray}                     & LOL            &                & SVM           & 0.975    \\
        \rowcolor{lightgray}\multirow{-3}{*}{15} & PCA            &                & SVM           & 0.975    \\
                                                 & KPCA           & Linear         & SVM           & 1.000    \\
                                                 & LOL            &                & LDA           & 1.000    \\
        \multirow{-3}{*}{30}                     & PCA            &                & SVM           & 1.000    \\
        \bottomrule
    \end{tabular}
    \caption{Performance Comparison of Dimensionality Reduction Techniques on the ORL dataset. (See Annex Table \ref{annex-tab:orl-dataset}) for full results.}
    \label{tab:orl-dataset}
\end{table}

% Comparison Line Chart
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\textwidth]{orl_line_chart.png}
    \caption{This figure shows how the accuracy of the best-performing model for each dimensionality reduction technique, evolves as the number of dimensions is increased.}
    \label{fig:orl_line_chart}
\end{figure}

\subsection{Explanation}

By looking at the scatter plots created by the different techniques in figures \ref{fig:orl-2d} and \ref{fig:orl-3d}, we can compare how well they perform in capturing and preserving the structure of the original dataset. Here are the key observations we can make:

\begin{itemize}
    \item The results from all the techniques, demonstrate a relatively even distribution of data points in the reduced space, indicating that it effectively captures the overall variance of the dataset.
    \item Only SLMVP with a radial kernel manages to clearly separate the classes. They do it in an interesting heart-like shape with 2 components.
\end{itemize}

% 2D Visualization
\begin{figure}[!h]
    \centering
    \includegraphics[width=1\textwidth]{orl_2d.png}
    \caption{Visualization of the clusters projected into 2 dimensions that resulted from the application of the different dimensionality reduction techniques on the ORL dataset.}
    \label{fig:orl-2d}
\end{figure}

% 3D Visualization
\begin{figure}[!h]
    \centering
    \includegraphics[width=\textwidth]{orl_3d.png}
    \caption{Visualization of the clusters projected into 3 dimensions that resulted from the application of the different dimensionality reduction techniques on the ORL dataset.}
    \label{fig:orl-3d}
\end{figure}

\subsubsection{Comparing Techniques}

The heat map in Figure \ref{fig:heatmap-orl} visually represents the similarity and dissimilarity between different dimensionality reduction techniques based on their Spearman rank correlation coefficients. Higher correlation coefficients indicate a stronger resemblance between the reduced-dimensional representations obtained using 2 dimensionality reduction techniques. We can make the following observations:

\begin{itemize}
    \item LLE and SLMVP exhibit the highest correlations between one another. This might be due to the fact that both techniques are local and incorporate neighborhood information of the dataset.
    \item KPCA, LLE, and LPP manifest moderate correlations.
\end{itemize}

% Heatmap
\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{heatmap_orl.png}[!h]
    \caption{Heatmap of Spearman Rank Correlation Coefficients}
    \label{fig:heatmap-orl}
\end{figure}

\subsubsection{Choosing Features and Components}

We can leverage the calculated absolute correlations to select the features that should be kept for a posterior machine learning task. Figure \ref{fig:hbar-orl} shows the 15 features with the highest absolute correlations with the first component, in the case of the first subplot, and the average of the first 5 components, in the case of the second subplot. For this image data, the correlations follow a smooth curve and it is difficult to determine how many features to reject. It seems reasonable to reject the features that correspond to the outer parts of the frame, but then again, those pixels might contain useful information.

\begin{figure}[!ht]
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{hbar_orl.png}
        \caption{1 component}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{hbar_orl_5dim.png}
        \caption{5 components}
    \end{subfigure}
    \caption{Original features of the ORL dataset ordered by their correlation to the components obtained with SLMVP with radial kernel and gamma values 0.1.}
    \label{fig:hbar-orl}
\end{figure}