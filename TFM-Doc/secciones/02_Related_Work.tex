\chapter{Related Work}

\section{Dimensionality Reduction}
Due to the massive technological advancements in processing power and data storage that have happened in the last few decades, data has grown exponentially in all areas of society. This abundance of data has introduced many challenges, including the curse of dimensionality. The curse of dimensionality refers to the many challenges that arise when working with high dimensional data. Some of those are: increased computational complexity, reduced interpretability, and the risk of overfitting.

Dimensionality reduction techniques have emerged as essential tools to address these issues by transforming high-dimensional data into a lower-dimensional space while preserving variability. Moreover, they facilitate the interpretation of high-dimensional datasets by identifying the key features that contribute most to the variability in the data.
This thesis provides an in-depth exploration of many of the most relevant dimensionality reduction techniques (PCA, KPCA, LOL, LPP, LLE and SLMVP).
%\cite{verleysen2005curse}

\subsection{Principal Component Analysis (PCA)}
%\cite{pearson1901principal}

% Introduction
Principal Component Analysis (PCA) is arguably the most popular and widely used dimensionality reduction technique. It aims at extracting the components that maximize the variance of the data in a lower dimensional space. Introduced by Karl Pearson in 1901, PCA has since been extensively applied in various fields, including signal processing and image processing. It has also been often used in the medical field, where patients datasets usually have a small sample size and a great number of features.

% How it works
PCA operates by identifying the principal components, which are orthogonal linear combinations of the original variables. These components capture the maximum variance in the data and are ordered in terms of their significance. The first principal component accounts for the largest proportion of the variance, followed by the subsequent components in descending order.

Mathematically, let $X$ be an $n \times m$ matrix, where each row represents a sample and each column corresponds to a feature. The first step in PCA is to calculate the covariance matrix $C$, which is computed as $C = \frac{X^T X}{N}$, capturing the relationships between different features.

% cite eigenvalue problem
Next, the eigenvectors and eigenvalues of the covariance matrix are calculated. Denoting the eigenvectors as $V$ and the eigenvalues as $\lambda$, we have $C V = \lambda V$. The eigenvectors $V$ represent the directions in the feature space along which the data exhibits the most significant variability, while the corresponding eigenvalues $\lambda$ indicate the amount of variance explained by each eigenvector.

To reduce the dimensionality of the data, a subset of the eigenvectors corresponding to the largest eigenvalues is selected. These eigenvectors form a new basis for the transformed data. The original dataset is then projected onto this reduced basis, resulting in a lower-dimensional representation, $P = V' X$, where $V'$ contains the selected eigenvectors.

% cite ISOMAP, LLP, Laplacian Eigenmaps and Locally Linear Embeddings
% Limitations
Despite its widespread use, PCA has certain limitations. It assumes a linear relationship between variables, and therefore struggles with cases in which the data clusters cannot be linearly separated. Moreover, it performs a global analysis on the whole dataset and does not preserve the local structure.
Extensions of PCA, such as kernel PCA, have been proposed to handle nonlinear relationships in the data. Other methods based on local structure preservation, such as ISOMAP, LLP, Laplacian Eigenmaps and Locally Linear Embeddings have been proposed to overcome the global characteristics of PCA.


\subsection{Kernel Principal Component Analysis (KPCA)}
%\cite{scholkopf1998nonlinear}
% Introduction
Kernel Principal Component Analysis (KPCA) is an extension of Principal Component Analysis (PCA) that addresses the limitations of linear methods by introducing a nonlinear mapping of the data into a higher-dimensional feature space. KPCA, proposed by Bernhard Schölkopf, Alexander Smola, and Klaus-Robert Müller in 1998, has since gained significant popularity in various domains, including computer vision, bioinformatics, and signal processing.

% How it works
KPCA aims to find a low-dimensional representation of the data while capturing the nonlinear structures and relationships present in the dataset. It achieves this by taking advantage of the kernel trick. The kernel trick involves defining a kernel function $K(x_i, x_j)$ that measures the similarity between two data points $x_i$ and $x_j$. Commonly used kernel functions include the Gaussian (RBF) kernel, polynomial kernel, and sigmoid kernel. By employing the kernel function, KPCA implicitly maps the data into a higher-dimensional feature space, which has the disadvantage that it increases the computational cost.

% PPT
Let $\phi : \mathbb{R}^N \rightarrow F$ be a (nonlinear) map. We refer to $F$ as the feature space. The covariance matrix is then
\begin{equation}
    C = \frac{\phi(X)^T \phi(X)}{N}
\end{equation}

Similar to PCA, the eigenvectors and eigenvalues of the covariance matrix are calculated. Denoting the eigenvectors as $V$ and the eigenvalues as $\lambda$, we have $C V = \lambda V$. The eigenvectors $V$.

To obtain the reduced-dimensional representation, a subset of the eigenvectors corresponding to the largest eigenvalues is selected. These eigenvectors, referred to as kernel principal components, define the transformed space in which the data is projected. The original dataset can then be mapped onto this space to obtain the lower-dimensional representation $P = V' \phi(X)$.

% Advantages over PCA, and limitations
KPCA offers several advantages over linear PCA. It can effectively capture nonlinear patterns and structures in the data, enabling a more accurate representation of complex relationships.
However, KPCA also comes with certain considerations. The choice of the kernel function and its associated parameters significantly impact the performance. Moreover, the computation of the kernel matrix can be computationally demanding because of the mapping to a higher-dimensional feature space.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{kernel_pca.png}
    \caption{Representation of the mapping function used in KPCA. \url{https://pdfs.semanticscholar.org/3f60/0e6c6cf93e78c9e6e690443d6d22c4bf18b9.pdf}}
    \label{fig:kernel_pca}
\end{figure}

\subsection{Linear Optimal Low-Rank (LOL)}
%\cite{doe2010linear}

Linear Optimal Low-Rank (LOL) is a dimensionality reduction technique that extends PCA by incorporating class-conditional means. This approach outperforms existing dimensionality reduction techniques and has demonstrated effectiveness with imaging and genetics data. LOL and SLMVP represent few of the supervised techniques that have been developed.

LOL was developed with the objective of enhancing the performance and accuracy of LDA, especially when the input data has a very large number of features (e.g. hundreds of millions), and a small sample size. This is known as the "large $p$, small $n$" problem, and it negatively affects the performance of classifiers, often making them overfit.

Mathematically, let $A \in \mathbb{R}^{d \times p}$ be a projection matrix i.e., the matrix that projects $p$-dimensional data into a $d$-dimensional subspace. We want to find the best projection matrix to pre-process the data before applying LDA. LOL suggests a matrix $A_{LOL}$ that is built by considering the eigenvectors of the class-conditionally centered covariance.

The fist step is to compute the sample mean of each class. They are ordered from highest to lowest, and then the differences of the means are calculated. For two classes: $\delta = \mu_0 - \mu_1$. For more than two classes: $\delta_i = \mu_i - \mu_1$ where $i \in {2,...,C}$ and $C$ is the number of classes. The class-centered covariance matrix is then calculated and the eigenvectors computed.

LOL offers several advantages over other dimensionality reduction techniques, its focus on improving classification accuracy when paired with a machine learning algorithm make it one of the best performing in that area.

\subsection{Locality Preserving Projection (LPP)}
% \cite
% He, Xiaofei, and Partha Niyogi. "Locality preserving projections." Advances in neural information processing systems 16 (2003).

Locality Preserving Projection (LPP) is a dimensionality reduction technique that aims to preserve the local structure and relationships among data points in a lower-dimensional space. LPP. In was introduced by Xiaofei He and Partha Niyogi in 2003.

It builds a graph incorporating neighborhood information
of the data set, that we will refer to as similarity matrix $W$ . Using the notion of the Laplacian of the graph, a transformation matrix which maps the data points to a subspace is computed. This linear transformation optimally preserves local neighborhood information.

Mathematically, let $\mathbf{X}$ be an $n \times m$ data matrix, where each row corresponds to a sample and each column represents a feature. LPP involves two key steps: constructing an similarity matrix $W$ and solving a generalized eigenvalue problem.

The first step is to construct a similarity matrix $W$ that encodes the pairwise similarities between data points. Commonly used similarity measures include the Gaussian kernel, k-nearest neighbors, or graph-based techniques. The matrix $W$ captures the local relationships and can be interpreted as a weighted adjacency matrix of a graph, where each data point is connected to its neighbors.

Given $W$, the goal is to find a projection matrix $\mathbf{P}$ that maps the data points into a lower-dimensional space. This is achieved by solving the generalized eigenvalue problem:

\begin{equation} \label{eq:1}
    X \mathbf{L} X^T a = \lambda XDX^Ta,
\end{equation}

where $\mathbf{L} = D-W$ is the Laplacian matrix. The matrix $\mathbf{P}$ consists of the eigenvectors corresponding to the smallest eigenvalues of equation \ref{eq:1}, representing the lower-dimensional representation of the data.

LPP offers several advantages over traditional dimensionality reduction techniques. It yields a map which is simple, linear, and defined everywhere. Furthermore, the algorithm can be easily kernelized and be made nonlinear.

However, LPP also has some considerations. The choice of the parameters when computing the similarity matrix (e.g. the number of neighbors $k$ if k-neares neighbors is used) can significantly change the result of applying LPP.

\subsection{LLE} %rewrite
% \cite{roweis2000nonlinear}

Locally Linear Embedding (LLE) is a powerful nonlinear dimensionality reduction technique that aims to preserve the local structure of the data in a lower-dimensional space. LLE, introduced by Sam T. Roweis and Lawrence K. Saul in 2000, has gained significant attention in various fields, including computer vision, data visualization, and manifold learning.

LLE addresses the limitations of linear projection methods by assuming that the data lies on a low-dimensional manifold that can be "unfolded". It seeks to find a representation of the data in a lower-dimensional space while preserving the local relationships between neighboring data points. By doing so, LLE is capable of capturing the underlying nonlinear structure of the data.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{lle_manifold.png}
    \caption{Illustration of the problem of nonlinear dimensionality reduction.}
    \label{fig:lle_manifold}
\end{figure}

Mathematically, let $X$ be an $n \times m$ data matrix, where each row corresponds to a sample and each column represents a feature. LLE involves three key steps: local reconstruction, weight determination, and global embedding.

The first step is local reconstruction, where for each data point $x_i$, the neighboring data points are identified. The goal is to reconstruct $x_i$ as a linear combination of its neighbors. This can be achieved by using an algorithm, such as the K-nearest neighbors.

The weight determination step involves solving the optimization problem to find the optimal weights. This can be done by minimizing the residual sum of squares.
\begin{equation}
    RSS(W) = \sum^n_{i=1} |X_i - \sum_{j \neq i} w_{ij} X_j|^2
\end{equation}
It holds $\sum_j W_{ij} = 1$, and $W_{ij} = 0$ if $X_j$ is not part of the neighborhood of $X_i$.

Finally, the global embedding step aims to find the low-dimensional representation of the data. This is achieved by, once again, minimizing the residual sum of squares but using the new weights to find the low-dimensional representation of the data points $Y$.
\begin{equation}
    \Phi(Y) = \sum^n_{i=1} |y_i - \sum_{j\neq i}w_{ij}Y_j|^2,
\end{equation}
where $\sum_i Y_{ij} = 0$, so that the data points are centered on the origin and subject to $Y Y^T = I$

The previous equation can me rewritten in its matrix multiplication form as
\begin{multline}
    \sum^n_{i=1} |y_i - \sum_{j \neq i} w_{ij} Y_j |^2 = \sum_i y_i^2 - y_i (\sum_j w_{ij}y_j) - (\sum_j w_{ij}y_j)y_i + (\sum_j w_{ij}y_j)^2 \\
    = Y^T T - Y^T (WY) - (WY)^T Y + (WY)^T (WY) \\
    = Y^T (I - W)^T (I-W)Y \\
    = Y^T M Y, \\
\end{multline}
and after applying Lagrange multipliers, the minimization obtained by solving the following eigenvalue problem.
\begin{equation} \label{eq:2}
    MY = \alpha Y
\end{equation}
The projection matrix $\mathbf{P}$ consists of the eigenvectors corresponding to the smallest eigenvalues of equation \ref{eq:2}, representing the lower-dimensional representation of the data.

LLE offers several advantages over linear projection methods. By preserving the local relationships, LLE can effectively capture the nonlinear structure of the data. It allows for the reduction of dimensionality while retaining the intrinsic properties and relationships of the data, enabling improved analysis and visualization.

However, LLE also has some considerations. The choice of the number of neighbors and the neighborhood size significantly impact the performance of LLE.

\subsection{Supervised Local Variance Maximum Preserving (SLMVP)}
Supervised Local Variance Maximum Preserving (SLMVP) is a dimensionality reduction technique that preserves the maximum local variance, and considers the distribution of the data by the response variable. SLMVP, proposed by Esteban García Cuesta et al., solves the "large p, small m" problem.

Mathematically, let $x_1, x_2, ..., x_m \in \mathfrak{R}^p$ be a set of inputs and $y_1, y_2, ..., y_m \in \mathfrak{R}^l$, where $m$ denotes the sample size, and $p$ and $l$ the number of input and output features respectively. SLMVP involves two key steps: constructing a similarity graph, and the unsupervised dimensionality reduction problem.

The first step, consists in the application of the similarity function $\mathscr{S}(X) : X \in \mathfrak{R}^{m \times p}$ to the inputs, and $\mathscr{S}(Y) : Y \in \mathfrak{R}^{m \times l}$ to the outputs. The application of these similarity functions defines an input weighted graph $\{H, U\}$ and an output weighted graph $\{I, V\}$, where $H$ and $I$ are nodes, and $U$ and $V$ vertices. The weights of the links represent the similarity between two data points. This allows the dimensionality reduction technique with the capability of being local.

The second step consist in solving the unsupervised dimensionality reduction problem, which aims to choose the mapping $y'_i = A^T x_i : y'_i \in \mathfrak{R}^k$ that minimizes the distance between neighbors in a multidimensional space. This can be expressed by the following cost function:

\begin{equation}\label{cost_1}
    J_{ns} = \frac{1}{2} \sum_{ij} || y'_i - y'_j || w_{ij},
\end{equation}

where $w_{ij}$ are the elements of matrix $W \in \mathfrak{R}^{m \times m}$, which is the similarity matrix $\mathscr{S}(X)$.
SLMVP solves the unsupervised dimensionality reduction problem  by choosing the mapping that  minimizes the distance between neighbors, preserving only those that are shared in the input and output spaces. The cost function \ref{cost_1} is then extended to be

\begin{equation}
    J_{ns} = \frac{1}{2} \sum_{ij} || y'_i - y'_j || z_{ij},
\end{equation}

where $z_{ij}$ are the elements of matrix $Z \in \mathfrak{R}^{m \times m}$, which represents the joint similarity matrix between input and output similarity matrices, being $z_{ij} = \sum^m_{k=1} u_{ik}v_{kj}$. This minimization can also be expressed in its kernelization form as the following maximization problem:

\begin{equation}
    \max tr(Y^T K_X K_Y Y)
\end{equation}

where $K_X = \mathscr{S}_X(X)$ and $K_Y \mathscr{S}_Y(Y)$ are the input and output similarity graphs represented as kernel functions. The maximization of the trace can be solved as the following eigenvector problem:

\begin{equation}
    X K_X K_Y X^T B = \lambda B
\end{equation}

where $B$ is the new reduced space.

\section{Explainable Artificial Intelligence}

As machine learning models, such as neural networks, become more sophisticated, they often operate as black boxes, making complex computations and generating results without transparently revealing the underlying processes to human users. This lack of explainability poses significant concerns in several domains. For instance, in healthcare, where AI is increasingly utilized for medical diagnoses or treatment recommendations, doctors and patients need to understand the rationale behind AI-driven decisions to ensure trust and accountability.
% Goals of XAI
Explainable Artificial Intelligence (XAI) aims at understanding machine learning model predictions and explain them in human and understandable terms to build trust with stakeholders.
% Interpretability vs Explainability
Interpretability focuses on model understanding techniques, while explainability focuses more broadly on model explanation and the interface for translating these explanations in human, understandable terms.

More complex models are difficult to debug, understand and control, thereby impeding their adoption.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{ai_dog.jpg}
    \caption{Example of explainable artificial intelligence in image recognition. It shows which regions contributed to the model's prediction of this image as a husky \url{https://cloud.google.com/blog/products/ai-machine-learning/explaining-model-predictions-on-image-data}}
    \label{fig:ai_dog}
\end{figure}

% rewrite
The field of Explainable Artificial Intelligence (XAI) is continuously evolving, and several state-of-the-art techniques have been developed to enhance the explainability of AI models. Here are a few notable advancements:

Interpretable Machine Learning: Techniques such as decision trees, rule-based models (e.g., rule lists, logical rules), and linear models provide inherent interpretability. They are widely used and have been enhanced to handle complex tasks and larger datasets.

Local Interpretable Model-Agnostic Explanations (LIME): LIME is a model-agnostic method that explains the predictions of any black-box model by creating a surrogate interpretable model. It perturbs the input data and observes the effect on the model's output, generating explanations for specific instances.

SHAP (SHapley Additive exPlanations): SHAP values are based on cooperative game theory and aim to allocate the "credit" or contribution of each feature in a prediction. SHAP provides a unified framework for interpreting the output of a wide range of models, including deep neural networks.

Integrated Gradients: This method assigns importance scores to input features by integrating gradients along a path from a baseline input to the input of interest. It quantifies how each feature contributes to the prediction and has been applied to explain deep learning models.

Attention Mechanisms: Attention mechanisms, commonly used in natural language processing and computer vision, allow models to focus on specific parts of the input. Attention weights provide insights into the model's decision-making process and highlight important regions or words.

Counterfactual Explanations: Counterfactual explanations generate alternative scenarios that could have led to a different outcome. They provide a "what-if" analysis and can be useful for explaining individual predictions or decisions.

Concept-based Explanations: These approaches aim to explain AI models in terms of human-understandable concepts or high-level features. They provide explanations at a more abstract level, enabling users to grasp the reasoning behind a model's output.

Model-specific Techniques: Some AI models have built-in explainability mechanisms. For example, decision trees can naturally provide interpretable explanations, and Bayesian networks can represent probabilistic relationships and support causal reasoning.

It's important to note that the effectiveness of these techniques may vary depending on the application domain, the specific AI model used, and the requirements of the end-users. The field of XAI is still rapidly evolving, and researchers are continuously working on developing more sophisticated and reliable methods to enhance the transparency and interpretability of AI systems.