\chapter{Introduction}

% TODO
% - Llevar el paragrafo 2 al final de la introducci√≥n.
% - Explicar que dimensionality reduction es el primer paso de una pipeline de machine learning
% - Mencionar B2, B4 de Jollife
%

As the availability of data continues to grow exponentially, the need for efficient and effective methods to reduce dimensionality and extract relevant information becomes more important.
% Feature Selection & Feature Extraction
Feature selection and feature extraction are two commonly used techniques in machine learning to reduce the dimensionality of a dataset. These techniques aim at identifying the most relevant features from the original set of input variables.
Feature selection tries to identify the subset that contains the most relevant and informative features while excluding redundant ones.
Feature extraction transforms the original set of features into a new set that is created by combining the original variables, thereby projecting the data onto a subspace of lower dimensionality. This process involves capturing the underlying structure and patterns of the data while reducing its dimensionality.

In this thesis, we explore various feature selection techniques including Principal Component Analysis (PCA), Kernel-PCA (KPCA), Linear Optimal Low Rank (LOL), Locality Preserving Projections (LPP), Locally Linear Embedding (LLE) and Supervised Local Maximum Variance Preserving (SLMVP).

There are two main binary classes into which these techniques can be classified: supervised or unsupervised, and local or not local.
% Supervised vs. Unsupervised
Supervised learning consists on training a model on labeled data, where the input samples have associated target or output labels, whereas unsupervised learning consists on training a model on unlabeled data, where the input samples do not have associated target or output labels. A lot of research in dimensionality reduction has been devoted to unsupervised techniques, including PCA, KPCA, LPP and LLE. This approach results in dimensions that have no statistical guarantee of being close to the best ones for classification. The recent supervised techniques LOL and SLMVP make it worth shifting the focus, since they outperform their unsupervised counterparts in several metrics.

% Local vs. Not Local
Local dimensionality reduction techniques focus on preserving the local structure and relationships among data points in the reduced feature space. They aim to maintain the similarities and dissimilarities among neighboring data points. LLE, LPP and SLMVP have this property, however SLMVP is the only dimensionality reduction technique that is both local and supervised.

% Introduce Explainability and connect it to the aim of this thesis
% TODO

% Introduction of objectives
% TODO

% List the objectives of the thesis
\begin{enumerate}
    \item Dig deep into SLMVP intricacies.
    \item Train SLMVP and other models and find the configuration of their parameters that is best for a classification task.
    \item Draw meaningful conclusions from the coefficients of the components that result from applying each of the dimensionality reduction techniques.
    \item Compare SLMVP against other state-of-the-art techniques on basis of the interpretation of their coefficients.
    \item Apply these techniques to real-world datasets.
\end{enumerate}

% List the main sections the thesis is organized into. With a very brief summary.
% TODO

\begin{enumerate}
    \item Introduction.
    \item Methodology: the experiments are described.
          \subitem Datasets: the datasets (ORL, COIL2000) are introduced.
\end{enumerate}