\chapter{Introduction}

% Feature Selection & Feature Extraction
Feature selection and feature extraction are two commonly used techniques in machine learning to reduce the dimensionality of a dataset. These techniques aim at identifying the most relevant features from the original set of input variables.

Feature selection tries to identify the subset that contains the most relevant and informative features while excluding redundant ones.

Feature extraction transforms the original set of features into a new set that is created by combining the original variables.

This thesis will be devoted to feature extraction techniques.

% Supervised vs. Unsupervised
Supervised learning involves training a model on labeled data, where the input samples have associated target or output labels.

Unsupervised learning involves training a model on unlabeled data, where the input samples do not have associated target or output labels.

% Local vs. Not Local
Local dimensionality reduction techniques focus on preserving the local structure and relationships among data points.


% List the objectives of the thesis
\begin{enumerate}
    \item Dig deep into SLMVP intricacies.
    \item Train SLMVP and other models and find the configuration of their parameters that is best for a classification task.
    \item Draw meaningful conclusions from the coefficients of the components that result from applying each of the dimensionality reduction techniques.
    \item Compare SLMVP against other state-of-the-art techniques on basis of the interpretation of their coefficients.
    \item Apply these techniques to real-world datasets.
\end{enumerate}

% List the main sections the thesis is organized into. With a very brief summary.
\begin{enumerate}
    \item Introduction.
    \item Methodology: the experiments are described.
          \subitem Datasets: the datasets (ORL, COIL2000) are introduced.
\end{enumerate}

\section{Dimensionality Reduction}
Due to the massive technological advancements in processing power and data storage that have happened in the last few decades, data in all areas of society has grown exponentially. This abundance of data has introduced many challenges, including the curse of dimensionality. The curse of dimensionality refers to the many challenges that arise when working with high dimensional data. Some of those are: increased computational complexity, reduced interpretability, and the risk of overfitting.

Dimensionality reduction techniques have emerged as essential tools to address these issues by transforming high-dimensional data into a lower-dimensional space while preserving variability. This thesis provides an in-depth exploration of many of the most relevant dimensionality reduction techniques (PCA, KPCA, LOL, LPP, LLE and SLMVP).
%\cite{verleysen2005curse}

\subsection{PCA}


\subsection{KPCA}


\subsection{LOL}


\subsection{LPP}


\subsection{LLE}


\subsection{SLMVP}

\section{Spearman's Rank Correlation Coefficient}
Spearman's rank correlation coefficient is a nonparametric measure of rank correlation. It measures the statistical dependence between the rankings of two variables. It is defined as the Pearson correlation coefficient between the rank variables. For variables $X$, $Y$ converted to ranks $R(X)$, $R(Y)$. The Spearman rank correlation coefficient is calculated in the following way.
\begin{equation}
    r_s = \rho_{R(X), R(Y)} = \frac{cov(R(X),R(Y))}{\sigma_{R(X)} \sigma_{R(Y)}}
\end{equation}


% 1. Introduction
% Introduction of all the dimensionality reduction techniques, leading to the introduction
% of SLMVP and its unique characteristics (e.g.: supervised, local) [SLMVP Paper Chapter 2]

% SLMVP solves the problem of LPP to work on problems with "large p small m"

% 2. Methodology

% Apply different dimensionality reduction techniques
% Get classification results
% Show with artificial data under what conditions SLMVP is better
%   than other techniques.

% Get correlation tables between original features and new dimensions.
% () Get eigenvalues
%%---------------------------------------------------------

% Maybe some background on using kernels?