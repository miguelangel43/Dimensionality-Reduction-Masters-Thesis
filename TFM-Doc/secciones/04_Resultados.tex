
\chapter{Results and Discussion}

Dimensionality reduction techniques prove particularly valuable when confronted with datasets with a high number of features. However, it is very challenging to explain and interpret components that are based on a multitude of features. Here, as in many areas of machine learning, there exists a trade-off between complexity and explainability. We try to navigate this trade-off by grouping features into categories when the datasets permits, as it happens in Section \ref{section:fifa}.

This sections shows the results of the evaluation of the dimensionality reduction techniques based on their performance in a classifying task, and proceeds to visually show and explain the components of the discovered subspaces by the best-performing techniques.

\section{Artificial Dataset}

\subsection{Classification Results}
In this section, the performance of various dimensionality reduction techniques is evaluated and compared on an artificially generated dataset. The aim is to identify the techniques that provide the best results in reducing the dimensionality of the dataset.
Table \ref{tab:artificial-dataset} presents the results obtained from evaluating different dimensionality reduction techniques. Each technique is assessed based on accuracy. The techniques are ranked in descending order of overall performance.

% Classification Table
\begin{table}[!ht]
    \begin{tabular}{llllr}
        \toprule
        {}                                      & Dim. Technique & Dim. Params        & Model         & Best Score \\
        Dimensions                              &                &                    &               &            \\
        \midrule
                                                & LLE            & k=30-reg=0.001     & XGBoost       & 0.90       \\
                                                & KPCA           & Linear             & XGBoost       & 0.63       \\
        \multirow{-3}{*}{1}                     & PCA            &                    & KNN           & 0.57       \\
        \rowcolor{lightgray}                    & SLMVP          & Linear             & LDA           & 0.96       \\
        \rowcolor{lightgray}                    & PCA            &                    & Random Forest & 0.96       \\
        \rowcolor{lightgray}\multirow{-3}{*}{2} & KPCA           & Linear             & KNN           & 0.96       \\
                                                & SLMVP          & Linear             & LDA           & 1.00       \\
                                                & LLE            & k=30-reg=0.001     & LDA           & 1.00       \\
        \multirow{-3}{*}{3}                     & PCA            &                    & LDA           & 1.00       \\
        \rowcolor{lightgray}                    & SLMVP          & Polynomial-Order=5 & LDA           & 1.00       \\
        \rowcolor{lightgray}                    & LLE            & k=30-reg=0.001     & KNN           & 1.00       \\
        \rowcolor{lightgray}\multirow{-3}{*}{4} & LPP            & k=17               & LDA           & 1.00       \\
                                                & SLMVP          & Linear             & LDA           & 1.00       \\
                                                & LLE            & k=30-reg=0.001     & KNN           & 1.00       \\
        \multirow{-3}{*}{5}                     & LPP            & k=17               & Random Forest & 1.00       \\
        \bottomrule
    \end{tabular}
    \caption{Performance Comparison of Dimensionality Reduction Techniques on an artificially generated dataset. This dataset contains 20 centers, 300 features and sample size 1000. (See Annex Table \ref{annex-tab:artificial-dataset}) for full results.}
    \label{tab:artificial-dataset}
\end{table}

% Comparison Line Chart
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\textwidth]{artificial_line_chart.png}
    \caption{This figure shows how the accuracy of the best-performing model for each dimensionality reduction technique, evolves as the number of dimensions is increased.}
    \label{fig:art_line_chart}
\end{figure}

For 2, 3, 4, and 5 dimensions, SLMVP, PCA and KPCA demonstrate the best performance among the tested dimensionality reduction techniques. They achieve an accuracy of 96\% in 2 dimensions and are able to correctly classify all the samples with 3 dimensions or more. Both SLMVP and KPCA achieve these results in their linear configurations, paired with the LLA classifier or K-Nearest Neighbors.
However, LLE emerges as the winner when only 1 dimension is allowed in the classification task. It achieves a 90\% accuracy paired with the XGBoost classifier, much higher than the second-best performing, which is KPCA with a 63\% accuracy.

Overall, SLMVP, KPCA and PCA demonstrate to be the most promising dimensionality reduction technique, especially when the classification task is not restricted to just 1 dimension. However, it is interesting to see how the results would change when applying the dimensionality reduction techniques to a real-world dataset.

\subsection{Explanation}

In this section, the results of applying various dimensionality reduction techniques are presented and compared using visual plots. The objective is to analyze and contrast the performance of these techniques in reducing the dimensionality of the dataset.

Figures \ref{fig:artificial_2d} and \ref{fig:artificial_3d} display a scatter plot of the dataset after applying the techniques. The plot shows the reduced-dimensional representation of the data, where the axes represent the principal components. By examining the scatter plot, we can observe the clustering and distribution of the data points in the reduced space.

% 2D Visualization
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{artificial_2d.png}
    \caption{Visualization of the clusters projected into 2 dimensions that resulted from the application of the different dimensionality reduction techniques on the artificially-generated data.}
    \label{fig:artificial_2d}
\end{figure}

% 3D Visualization
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{artificial_3d.png}
    \caption{Visualization of the clusters projected into 3 dimensions that resulted from the application of the different dimensionality reduction techniques on the artificially-generated data.}
    \label{fig:artificial_3d}
\end{figure}

By examining the scatter plots obtained from the different dimensionality reduction techniques, we can compare their performance in capturing and preserving the structure of the original dataset. The following observations can be made:

\begin{itemize}
    \item SLMVP with a linear kernel, PCA and LPP demonstrate a relatively even distribution of data points in the reduced space, indicating that it effectively captures the overall variance of the dataset. However, it may not be able to capture complex nonlinear relationships.
    \item SLMVP and LOL exhibit a clear separation between different classes in the reduced space, even though this is not clearly visible in the LOL plot, since a few clusters lay very far away from where most of them are located. These are clearly separated although they appear to be overlapping. The notable class separation in the plots corresponding to SLMVP and LOL emphasizes the discriminative power between classes, making it useful for classification tasks.
    \item SLMVP with a radial kernel captures the classes in a curve, which can be clearly seen in its 3-dimensional representation (Figure \ref{fig:artificial_3d}).
\end{itemize}

\subsubsection{Comparing Techniques}

% Heatmap
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\textwidth]{heatmap_artificial.png}
    \caption{Heatmap of Spearman Rank Correlation Coefficients}
    \label{fig:heatmap-artificial}
\end{figure}


The heatmap in Figure \ref{fig:heatmap-artificial} visually represents the similarity and dissimilarity between different dimensionality reduction techniques based on their Spearman rank correlation coefficients. The correlation coefficients are calculated by measuring the absolute correlations between the data in the reduced space from each technique and the original data. Higher correlation coefficients indicate a stronger resemblance between the reduced-dimensional representations obtained using 2 dimensionality reduction techniques.
The color and the color intensity in the heatmap represent the magnitude of the correlation coefficients. Darker red shades indicate higher correlation coefficients, indicating a stronger resemblance between the reduced-dimensional representations. Darker blue shades, on the other hand, suggest weaker correlations. We can make the following observations.

\begin{itemize}
    \item SLMVP is most strongly-correlated with LPP and LOL, although this correlation is only moderate, below 60\%. The correlation with LPP could be due to the fact that both SLMVP and LPP are local and therefore incorporate neighborhood information of the dataset. On the other hand, the correlation with LOL could be due to another property that both SLMVP and LOL posses, the fact that both are supervised and therefore incorporate information about the dependent variable.
    \item The strongest correlation over all is between PCA and LOL with 92\%, and the lowest between LLE and LOL with 10\%.
\end{itemize}

\subsubsection{Multilabel}

% Multilabel
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{2D-multilabel.png}
    \caption{Visualization of the clusters projected into 2 dimensions that resulted from the application of the different dimensionality reduction techniques on artificially-generated multilabel data.}
    \label{fig:2D_multilabel}
\end{figure}

Among the supervised dimensionality reduction techniques evaluated, SLMVP stands out as the sole method capable of effectively handling multilabel datasets. This characteristic holds great significance due to the important role that information about the relationships between multiple classes and variables plays in preserving the intrinsic structure of the data during dimensionality reduction. By leveraging SLMVP's unique ability to capture and retain these intricate connections, it ensures a more accurate and comprehensive representation of the data, thereby enhancing the effectiveness of dimensionality reduction tasks.

Figure \ref{fig:2D_multilabel} shows the reduced-dimensional representation of a 2-label dataset. It is worth noting that LOL is not capable of handling multilabel data and is therefore not on the plot. The figure shows that only SLMVP is capable of clearly separating the datapoints into their different four combinations of classes.

\subsubsection{Choosing Features and Components}

\begin{figure}[!ht]
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{hbar_artificial.png}
        \caption{A subfigure}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{hbar_artificial_5dim.png}
        \caption{A subfigure}
    \end{subfigure}
    \caption{Original features of the dataset ordered by their correlation to the components obtained with SLMVP with radial kernel and gamma values 0.1.}
    \label{fig:test}
\end{figure}


\section{FIFA Dataset}\label{section:fifa}

\subsection{Classification Results}

In this section, we assess and compare different techniques for reducing the dimensionality of the FIFA dataset introduced in section \ref{section:fifa-dataset}. Our goal is to identify the techniques that provide the best results in terms of reducing the dataset's complexity. We present the evaluation results in Table \ref{tab:fifa-dataset}, which ranks the techniques based on their accuracy. The table shows the best-performing techniques at the top, and the order goes down from there.

% Classification Table
\begin{table}[!ht]
    \begin{tabular}{llllr}
        \toprule
        {}                                      & Dim. Technique & Dim. Params       & Model         & Accuracy \\
        Dimensions                              &                &                   &               &          \\
        \midrule
                                                & LLE            & k=81-reg=0.001    & Random Forest & 0.889041 \\
                                                & SLMVP          & Linear            & KNN           & 0.871233 \\
        \multirow{-3}{*}{1}                     & LOL            &                   & LDA           & 0.846575 \\
        \rowcolor{lightgray}                    & SLMVP          & Radial-Gammas=0.1 & Decision Tree & 0.931507 \\
        \rowcolor{lightgray}                    & LLE            & k=81-reg=0.001    & Random Forest & 0.895890 \\
        \rowcolor{lightgray}\multirow{-3}{*}{2} & LOL            &                   & Random Forest & 0.876712 \\
                                                & SLMVP          & Radial-Gammas=0.1 & KNN           & 0.934247 \\
                                                & LLE            & k=81-reg=0.001    & XGBoost       & 0.932877 \\
        \multirow{-3}{*}{3}                     & PCA            &                   & XGBoost       & 0.906849 \\
        \rowcolor{lightgray}                    & LLE            & k=81-reg=0.001    & XGBoost       & 0.946575 \\
        \rowcolor{lightgray}                    & SLMVP          & Radial-Gammas=0.1 & LDA           & 0.931507 \\
        \rowcolor{lightgray}\multirow{-3}{*}{4} & LPP            & k=5               & KNN           & 0.910959 \\
                                                & LLE            & k=81-reg=0.001    & XGBoost       & 0.949315 \\
                                                & SLMVP          & Radial-Gammas=0.1 & Random Forest & 0.932877 \\
        \multirow{-3}{*}{5}                     & LPP            & k=5               & KNN           & 0.923288 \\
        \bottomrule
    \end{tabular}
    \caption{Performance Comparison of Dimensionality Reduction Techniques on the FIFA dataset. (See Annex Table \ref{annex-tab:fifa-dataset}) for full results.}
    \label{tab:fifa-dataset}
\end{table}

% Comparison Line Chart
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\textwidth]{fifa_line_chart.png}
    \caption{This figure shows how the accuracy of the best-performing model for each dimensionality reduction technique, evolves as the number of dimensions is increased.}
    \label{fig:fifa_line_chart}
\end{figure}


\subsection{Explanation}

% 2D Visualization
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{fifa_2d.png}
    \caption{Visualization of the clusters projected into 2 dimensions that resulted from the application of the different dimensionality reduction techniques on the FIFA dataset.}
    \label{fig:fifa_2d}
\end{figure}

% 3D Visualization
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{fifa_3d.png}
    \caption{Visualization of the clusters projected into 3 dimensions that resulted from the application of the different dimensionality reduction techniques on the FIFA dataset.}
    \label{fig:fifa_3d}
\end{figure}

By looking at the scatter plots created by the different techniques in figure \ref{fig:fifa_2d} and \ref{fig:fifa_3d}, we can compare how well they perform in capturing and preserving the structure of the original dataset. Here are the key observations we can make:

\begin{itemize}
    \item The results from all the techniques except KPCA with a radial kernel, demonstrate a relatively even distribution of data points in the reduced space, indicating that it effectively captures the overall variance of the dataset.
    \item SLMVP with linear and polynomial kernels, PCA and LPP manage to separate the three different classes (corresponding to attacker, defender and midfielder) in a single component. In the case of SLMVP with a linear kernel and PCA, it is the first component the one that captures the separation, whereas the second component captures the variability of the data points along other features that do not seem to have such a great impact on the dependent variable.
    \item As will be shown later, the axes that capture the separation of the classes show a strong correlation with features related to attacking and defending features. Either a positive correlation with attacking and a negative correlation with defending, or a negative correlation with attacking and a positive correlation with defending.
    \item The dimensionality reduction techniques do a great job clearly separating the classes \textit{attacker} and \textit{defender}. However the class \textit{midfielder} lies between the two other classes, often overlapping with them.
\end{itemize}

\subsubsection{Comparing Techniques}

% Heatmap
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\textwidth]{heatmap_fifa.png}
    \caption{Heatmap of Spearman Rank Correlation Coefficients}
    \label{fig:heatmap-fifa}
\end{figure}

The heatmap in Figure \ref{fig:heatmap-fifa} visually represents the similarity and dissimilarity between different dimensionality reduction techniques based on their Spearman rank correlation coefficients. The correlation coefficients are calculated by measuring the absolute correlations between the data in the reduced space from each technique and the original data. Higher correlation coefficients indicate a stronger resemblance between the reduced-dimensional representations obtained using 2 dimensionality reduction techniques. We can make the following observations:

\begin{itemize}
    \item SLMVP is most strongly-correlated with LPP and LOL, although this correlation is only moderate, below 60\%. The correlation with LPP could be due to the fact that both SLMVP and LPP are local and therefore incorporate neighborhood information of the dataset. On the other hand, the correlation with LOL could be due to another property that both SLMVP and LOL posses, the fact that both are supervised and therefore incorporate information about the dependent variable.
    \item The strongest correlation over all is between PCA and LOL with 96\%, and the lowest between LLE and LOL with 10\%.
\end{itemize}

\subsubsection{Explaining Components}

As mentioned earlier in this section, dimensionality reduction techniques are the most effective when applied to datasets with a large number of features. However, explaining a dataset with many features is challenging, due to the difficulty of showing high-dimensional data in graphs that humans can read. The FIFA dataset helps us address that challenge. The features provide insights about a soccer player's ability and they can be grouped into 6 categories: defending, attacking, skill, power, movement and mentality. For example, the features \textit{attacking\_crossing}, \textit{attacking\_finishing}, \textit{attacking\_heading\_accuracy}, \textit{attacking\_short\_passing}, and \textit{attacking\_volleys} are grouped as attacking.

As explained in section \ref{exp_comp}, the correlations between the input data represented in its original features and the data projected onto the first reduced dimensions is calculated. For the sake of a good visualization, the correlations concerning the features of each of the aforementioned categories are grouped and averaged. For example, \textit{attacking} in Figure \ref{fig:spider_fifa} shows the average of the correlations of \textit{attacking\_crossing}, \textit{attacking\_finishing}, etc, with the first or second component. This is a way of bringing a muti-dimensional problem to a number of variables that we are more comfortable drawing insights from.

% Spider
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{spider_fifa_2_comp.png}
    \caption{Visualization of the average correlation of the first two components with the features grouped by skill set.}
    \label{fig:spider_fifa}
\end{figure}

By looking at the spider plots created by the different techniques in Figure \ref{fig:spider_fifa}, we can make some key observations and explain the components:

\begin{itemize}
    \item Every dimensionality reduction technique, with the exception of LLE, shows high correlation coefficients for \textit{defending}. This feature is key to separate the classes attackers and defenders, because the first shows a high negative correlation with \textit{defending} and the latter shows a high positive correlation. Intuitively this makes sense, as defenders are better at defending than attackers.
    \item LOL's first two components are very similar in their correlations to the original features.
    \item PCA's and LOL's spider plot concerning the first component (marked in blue in the graph) are very similar. This matches what was shown in the heat map of Figure \ref{fig:heatmap-fifa}, where they shared a 96\% Spearman Rank Correlation Coefficient.
\end{itemize}

% 2D -  Spider
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{slmvp_2d_spider.png}
    \caption{Visualization of the average correlation of the first two components with the features grouped by skill set.}
    \label{fig:2d_spider_fifa}
\end{figure}

% TODO: Insights
Now we select the results of \textit{SLMVP Radial-Gammas=0.01} in order to give detailed explanations of their components. Figure \ref{fig:2d_spider_fifa} shows side-by-side a scatterplot with the data points projected into the first 2 components, and a spider plot that conveys how these two components relate to the original features of the dataset. The following insights are drawn:

\begin{itemize}
    \item The first component is positively correlated with \textit{defending}. It separates the attackers from the defenders and midfielders. Attackers are shown to have less defending skills than the other groups.
    \item The second component is positively correlated with \textit{attacking} and \textit{skill}, and negatively correlated with \textit{defending}. It separates the defenders from the attackers and midfielders.
\end{itemize}



% \begin{table}[!ht]
%     \begin{tabular}{lrr}
%         \toprule
%         {}        & SLMVP Component 1  & SLMVP Component 2  \\
%         skill set &                    &                    \\
%         \midrule
%         attacking & \textbf{-0.128595} & \textbf{0.348037}  \\
%         defending & \textbf{0.619712}  & \textbf{-0.651872} \\
%         mentality & 0.015077           & 0.191322           \\
%         movement  & 0.072015           & 0.316958           \\
%         power     & -0.191797          & 0.094459           \\
%         skill     & 0.146032           & 0.455555           \\
%         \bottomrule
%     \end{tabular}
%     \caption{}
%     \label{tab:fifa-corr}
% \end{table}

% \begin{figure}[!ht]
%     \centering
%     \includegraphics[width=0.4\textwidth]{spider_SLMVP_radial.png}
%     \label{fig:spider_slmvp_fifa}
% \end{figure}

\subsubsection{Choosing Features and Components}

\begin{figure}[!ht]
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{hbar_fifa.png}
        \caption{A subfigure}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{hbar_fifa_5dim.png}
        \caption{A subfigure}
    \end{subfigure}
    \caption{Original features of the dataset ordered by their correlation to the components obtained with SLMVP with radial kernel and gamma values 0.1.}
    \label{fig:test}
\end{figure}



\section{Insurance Company Benchmark (COIL 2000) Dataset}

In this section, we conduct an evaluation and comparison of various techniques aimed at reducing the dimensionality of the FIFA dataset introduced in section \ref{section:coil-dataset}. Our objective is to determine the most effective techniques for minimizing the dataset's complexity. The evaluation results are presented in Table \ref{tab:coil-dataset}, where the techniques are ranked based on their accuracy. The table showcases the top-performing techniques in descending order.

\subsection{Classification Results}

% Classification Table
\begin{table}[!h]
    \begin{tabular}{llllr}
        \toprule
        {}                                       & Dim. Technique & Dim. Params        & Model         & Accuracy \\
        Dimensions                               &                &                    &               &          \\
        \midrule
                                                 & SLMVP          & Radial-Gammas=0.01 & KNN           & 0.760    \\
                                                 & LOL            &                    & Naive Bayes   & 0.730    \\
        \multirow{-3}{*}{1}                      & KPCA           & Linear             & Naive Bayes   & 0.700    \\
        \rowcolor{lightgray}                     & SLMVP          & Radial-Gammas=0.01 & LDA           & 0.775    \\
        \rowcolor{lightgray}                     & LOL            &                    & Naive Bayes   & 0.735    \\
        \rowcolor{lightgray}\multirow{-3}{*}{3}  & LLE            & k=28-reg=0.001     & Naive Bayes   & 0.715    \\
                                                 & SLMVP          & Linear             & Naive Bayes   & 0.750    \\
                                                 & LOL            &                    & Naive Bayes   & 0.725    \\
        \multirow{-3}{*}{5}                      & KPCA           & Linear             & AdaBoost      & 0.715    \\
        \rowcolor{lightgray}                     & SLMVP          & Radial-Gammas=0.01 & KNN           & 0.740    \\
        \rowcolor{lightgray}                     & LOL            &                    & Random Forest & 0.710    \\
        \rowcolor{lightgray}\multirow{-3}{*}{15} & KPCA           & Radial-Gamma=0.007 & Naive Bayes   & 0.705    \\
                                                 & SLMVP          & Linear             & KNN           & 0.755    \\
                                                 & LLE            & k=28-reg=0.001     & LDA           & 0.740    \\
        \multirow{-3}{*}{30}                     & LPP            & k=11               & KNN           & 0.725    \\
        \bottomrule
    \end{tabular}
    \caption{Performance Comparison of Dimensionality Reduction Techniques on the COIL 2000 dataset. (See Annex Table \ref{annex-tab:coil-dataset}) for full results.}
    \label{tab:coil-dataset}
\end{table}

% Comparison Line Chart
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\textwidth]{coil_line_chart.png}
    \caption{This figure shows how the accuracy of the best-performing model for each dimensionality reduction technique, evolves as the number of dimensions is increased.}
    \label{fig:coil_line_chart}
\end{figure}

\subsection{Visualization}
% https://online.stat.psu.edu/stat505/lesson/11/11.4

% 2D Visualization
\begin{figure}[!h]
    \centering
    \includegraphics[width=\textwidth]{coil2000_2d.png.png}
    \caption{Visualization of the clusters projected into 2 dimensions that resulted from the application of the different dimensionality reduction techniques on the COIL 2000 dataset.}
    \label{fig:coil-2d}
\end{figure}

% 3D Visualization
\begin{figure}[!h]
    \centering
    \includegraphics[width=\textwidth]{coil2000_3d.png.png}
    \caption{Visualization of the clusters projected into 3 dimensions that resulted from the application of the different dimensionality reduction techniques on the COIL 2000 dataset.}
    \label{fig:coil-3d}
\end{figure}

% Heatmap
\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{heatmap_coil.png}[!h]
    \caption{Heatmap of Spearman Rank Correlation Coefficients}
    \label{fig:heatmap-coil}
\end{figure}

\subsubsection{Choosing Features and Components}

\begin{figure}[!ht]
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{hbar_coil.png}
        \caption{A subfigure}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{hbar_coil_5dim.png}
        \caption{A subfigure}
    \end{subfigure}
    \caption{Original features of the dataset ordered by their correlation to the components obtained with SLMVP with radial kernel and gamma values 0.1.}
    \label{fig:test}
\end{figure}

\section{ORL Dataset}

\subsection{Classification Results}

% Classification Table
\begin{table}[!h]
    \begin{tabular}{llllr}
        \toprule
        {}                                       & Dim. Technique & Dim. Params    & Model         & Accuracy \\
        Dimensions                               &                &                &               &          \\
        \midrule
                                                 & LLE            & k=18-reg=0.001 & KNN           & 0.300    \\
                                                 & KPCA           & Linear         & Random Forest & 0.200    \\
        \multirow{-3}{*}{1}                      & PCA            &                & XGBoost       & 0.175    \\
        \rowcolor{lightgray}                     & KPCA           & Linear         & SVM           & 0.750    \\
        \rowcolor{lightgray}                     & LLE            & k=18-reg=0.001 & SVM           & 0.750    \\
        \rowcolor{lightgray}\multirow{-3}{*}{3}  & PCA            &                & SVM           & 0.750    \\
                                                 & LOL            &                & SVM           & 0.925    \\
                                                 & KPCA           & Linear         & SVM           & 0.900    \\
        \multirow{-3}{*}{5}                      & PCA            &                & SVM           & 0.900    \\
        \rowcolor{lightgray}                     & KPCA           & Linear         & SVM           & 0.975    \\
        \rowcolor{lightgray}                     & LOL            &                & SVM           & 0.975    \\
        \rowcolor{lightgray}\multirow{-3}{*}{15} & PCA            &                & SVM           & 0.975    \\
                                                 & KPCA           & Linear         & SVM           & 1.000    \\
                                                 & LOL            &                & LDA           & 1.000    \\
        \multirow{-3}{*}{30}                     & PCA            &                & SVM           & 1.000    \\
        \bottomrule
    \end{tabular}
    \caption{Performance Comparison of Dimensionality Reduction Techniques on the ORL dataset. (See Annex Table \ref{annex-tab:orl-dataset}) for full results.}
    \label{tab:orl-dataset}
\end{table}

% Comparison Line Chart
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\textwidth]{orl_line_chart.png}
    \caption{This figure shows how the accuracy of the best-performing model for each dimensionality reduction technique, evolves as the number of dimensions is increased.}
    \label{fig:orl_line_chart}
\end{figure}

\subsection{Visualization}

% 2D Visualization
\begin{figure}[!h]
    \centering
    \includegraphics[width=1\textwidth]{orl_2d.png}
    \caption{Visualization of the clusters projected into 2 dimensions that resulted from the application of the different dimensionality reduction techniques on the ORL dataset.}
    \label{fig:orl-2d}
\end{figure}

% 3D Visualization
\begin{figure}[!h]
    \centering
    \includegraphics[width=\textwidth]{orl_3d.png}
    \caption{Visualization of the clusters projected into 3 dimensions that resulted from the application of the different dimensionality reduction techniques on the ORL dataset.}
    \label{fig:orl-3d}
\end{figure}

% Heatmap
\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{heatmap_orl.png}[!h]
    \caption{Heatmap of Spearman Rank Correlation Coefficients}
    \label{fig:heatmap-orl}
\end{figure}

\subsubsection{Choosing Features and Components}

\begin{figure}[!ht]
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{hbar_orl.png}
        \caption{A subfigure}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{hbar_orl_5dim.png}
        \caption{A subfigure}
    \end{subfigure}
    \caption{Original features of the dataset ordered by their correlation to the components obtained with SLMVP with radial kernel and gamma values 0.1.}
    \label{fig:test}
\end{figure}