
\chapter{Results and conclusions}

Challenge: dimensionality reduction is interesting to use when there are great number of features, but it is easier to explain a model that has few features. FIFA dataset overcomes this issue.


\section{Artificial Dataset}

\subsection{Classification Results}


\begin{tabular}{p{2cm}p{2.5cm}lrp{0.3\linewidth}}
    \toprule
    Dim. \newline Technique                     & Number of \newline Dimensions & Model         & Best Score & Params                                                  \\
    \midrule
                                                & 1                             & XGBoost       & 0.63       & \{'xgb\_\_n\_estimators': 5\}                           \\
                                                & 2                             & KNN           & 0.96       & \{'knn\_\_n\_neighbors': 50\}                           \\
                                                & 3                             & LDA           & 1.00       & \{'lda\_\_solver': 'svd'\}                              \\
                                                & 4                             & LDA           & 1.00       & \{'lda\_\_solver': 'svd'\}                              \\
    \multirow{-5}{*}{KPCA}                      & 5                             & LDA           & 1.00       & \{'lda\_\_solver': 'svd'\}                              \\
    \rowcolor{lightgray}                        & 1                             & XGBoost       & 0.90       & \{'xgb\_\_n\_estimators': 5\}                           \\
    \rowcolor{lightgray}                        & 2                             & KNN           & 0.91       & \{'knn\_\_n\_neighbors': 20\}                           \\
    \rowcolor{lightgray}                        & 3                             & LDA           & 1.00       & \{'lda\_\_solver': 'lsqr'\}                             \\
    \rowcolor{lightgray}                        & 4                             & KNN           & 1.00       & \{'knn\_\_n\_neighbors': 3\}                            \\
    \rowcolor{lightgray}\multirow{-5}{*}{LLE}   & 5                             & KNN           & 1.00       & \{'knn\_\_n\_neighbors': 3\}                            \\
                                                & 1                             & LDA           & 0.34       & \{'lda\_\_solver': 'svd'\}                              \\
                                                & 2                             & LDA           & 0.64       & \{'lda\_\_solver': 'svd'\}                              \\
                                                & 3                             & LDA           & 0.85       & \{'lda\_\_solver': 'svd'\}                              \\
                                                & 4                             & Naive Bayes   & 0.95       & \{\}                                                    \\
    \multirow{-5}{*}{LOL}                       & 5                             & KNN           & 0.99       & \{'knn\_\_n\_neighbors': 3\}                            \\
    \rowcolor{lightgray}                        & 1                             & Decision Tree & 0.47       & \{'dt\_\_max\_depth': 10\}                              \\
    \rowcolor{lightgray}                        & 2                             & Random Forest & 0.89       & \{'rf\_\_max\_depth': 10, 'rf\_\_n\_estimators': 100\}  \\
    \rowcolor{lightgray}                        & 3                             & LDA           & 0.96       & \{'lda\_\_solver': 'svd'\}                              \\
    \rowcolor{lightgray}                        & 4                             & LDA           & 1.00       & \{'lda\_\_solver': 'svd'\}                              \\
    \rowcolor{lightgray}\multirow{-5}{*}{LPP}   & 5                             & Random Forest & 1.00       & \{'rf\_\_max\_depth': None, 'rf\_\_n\_estimators': 50\} \\
                                                & 1                             & KNN           & 0.57       & \{'knn\_\_n\_neighbors': 50\}                           \\
                                                & 2                             & Random Forest & 0.96       & \{'rf\_\_max\_depth': None, 'rf\_\_n\_estimators': 50\} \\
                                                & 3                             & LDA           & 1.00       & \{'lda\_\_solver': 'svd'\}                              \\
                                                & 4                             & LDA           & 1.00       & \{'lda\_\_solver': 'svd'\}                              \\
    \multirow{-5}{*}{PCA}                       & 5                             & LDA           & 1.00       & \{'lda\_\_solver': 'svd'\}                              \\
    \rowcolor{lightgray}                        & 1                             & XGBoost       & 0.56       & \{'xgb\_\_n\_estimators': 10\}                          \\
    \rowcolor{lightgray}                        & 2                             & LDA           & 0.96       & \{'lda\_\_solver': 'svd'\}                              \\
    \rowcolor{lightgray}                        & 3                             & LDA           & 1.00       & \{'lda\_\_solver': 'svd'\}                              \\
    \rowcolor{lightgray}                        & 4                             & LDA           & 1.00       & \{'lda\_\_solver': 'svd'\}                              \\
    \rowcolor{lightgray}\multirow{-5}{*}{SLMVP} & 5                             & LDA           & 1.00       & \{'lda\_\_solver': 'svd'\}                              \\
    \bottomrule
\end{tabular}

\subsection{Visualization}
% Comparison Line Chart
\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{artificial_line_chart.png}
    \caption{This figure shows how the accuracy of the best-performing model for each dimensionality reduction technique, evolves as the number of dimensions is increased.}
    \label{fig:art_line_chart}
\end{figure}

% 2D Visualization
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{artificial_2d.png}
    \caption{Visualization of the clusters projected into 2 dimensions that resulted from the application of the different dimensionality reduction techniques on the artificially-generated data.}
    \label{fig:artificial_2d}
\end{figure}

% 3D Visualization
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{artificial_3d.png}
    \caption{Visualization of the clusters projected into 3 dimensions that resulted from the application of the different dimensionality reduction techniques on the artificially-generated data.}
    \label{fig:artificial_3d}
\end{figure}

% Spearman's Rank Correlation - Heatmap
\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{heatmap_artificial.png}
    \caption{Visualization of }
    \label{fig:heatmap}
\end{figure}

% Multilabel
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{2D-multilabel.png}
    \caption{Visualization of the clusters projected into 2 dimensions that resulted from the application of the different dimensionality reduction techniques on artificially-generated multilabel data.}
    \label{fig:2D_multilabel}
\end{figure}

\section{Insurance Company Benchmark (COIL 2000) Dataset}

\subsection{Classification Results}

\subsection{Interpretation of Coefficients}
% https://online.stat.psu.edu/stat505/lesson/11/11.4


\section{ORL Dataset}

\subsection{Classification Results}

\subsection{Visual Comparison}

\begin{figure}
    \centering
    \includegraphics[width=1.1\textwidth]{orl1.png}
    \caption{Visualization of }
    \label{fig:orl}
\end{figure}

\section{FIFA Dataset}

\subsection{Classification Results}

\subsection{Interpretation of Coefficients}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{spider_fifa.png}
    \caption{Visualization of }
    \label{fig:spider_fifa}
\end{figure}

% https://online.stat.psu.edu/stat505/lesson/11/11.4