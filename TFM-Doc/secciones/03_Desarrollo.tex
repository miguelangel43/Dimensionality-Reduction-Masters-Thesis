\chapter{Methodology}
This section explains the experimental setup, interpretation of components, and the process of selecting features and components.

% \section{Spearman's Rank Correlation Coefficient} \label{section:spearman}
% Spearman's rank correlation coefficient is a nonparametric measure of rank correlation. It measures the statistical dependence between the rankings of two variables. It is defined as the Pearson correlation coefficient between the rank variables. For variables $X$, $Y$ converted to ranks $R(X)$, $R(Y)$. The Spearman rank correlation coefficient is calculated in the following way.
% \begin{equation}
%     r_s = \rho_{R(X), R(Y)} = \frac{cov(R(X),R(Y))}{\sigma_{R(X)} \sigma_{R(Y)}}
% \end{equation}

\section{Interpretation of Components}
We will provide an explanation of principal components in two dimensions. First, we will determine their relationship with the original features of the data. Second, we will examine how the components obtained through one technique relate to those obtained through another technique. The methods behind how these questions are addressed will be explained in detail to provide a comprehensive understanding.

\subsection{Explaining Components}\label{exp_comp}

Before diving into the step-by-step process, it is essential to define some variables that are present in dimensionality reduction tasks: let the input data $X$ be an $n \times p$ matrix, where each row represents a sample and each column, denoted as $X_1, X_2, ..., X_p$, corresponds to a feature. Furthermore, let $V \in \mathbb{R}^{d \times p}$ be the projection matrix, i.e., the matrix that projects $p$-dimensional data into a $d$-dimensional subspace and let $V_i$ represent the $i$-th column of $V$ sorted in descending order by their significance (e.g., by the magnitude of their eigenvalues or the variability they capture). Finally let $P \in \mathbb{R}^{n \times d}$ be the the projection of the data into the $d$-dimensional space, so that $P = XV$ holds, with columns $P_1, P_2,..., P_d$.

The first step is to calculate the correlation between the input data represented in its original features $X_1, X_2, ..., X_p$ and the data projected onto the reduced $d$-dimensions $P_1, P_2, ..., P_d$. Let $r_i$ be the vector that contains the correlations between the input data $X$ and its projection onto the $i$-th dimension $P_i$.

$$
    r_i =
    \begin{bmatrix}
        corr(P_i, X_1) \\
        corr(P_i, X_2) \\
        ...            \\
        corr(P_i, X_p)
    \end{bmatrix}
$$

This vector of correlations will be utilized to explain a component based on the original features. We could now make statements such as: "component 1 is positively correlated with feature $X_1$, negatively correlated with $X_3$ and does not show any other strong correlations", and draw insights from how the data is distributed along this the axis that this component represents.

\subsection{Comparing Techniques}

We can also leverage the correlations between the dimensions generated by the technique and the original features to compare the similarity and dissimilarity of the components obtained applying different techniques.
Correlation can be negative or positive. However, it is the absolute value what determines the degree of significance of the correlation. Therefore, we take the absolute values $|corr(P_i,X_j)|$. Moreover, the difference between the value of correlations is not uniform among the different dimensionality reduction techniques and it prevents us from doing a fair comparison. To address that, we take the position of the absolute correlation in a list that is sorted in ascending order. Let $index(j)$ be the index such that $\#\{i : x_i \leq x_j\} = \lfloor j \rfloor$ for $i, j \in \{1,2,...,p\}$.

$$
    s_i =
    \begin{bmatrix}
        index(|corr(P_i, X_1)|) \\
        index(|corr(P_i, X_2)|) \\
        ...                     \\
        index(|corr(P_i, X_p)|)
    \end{bmatrix}
$$

For example, suppose we have some input data with 3 features $X_1, X_2$ and $X_3$ and we would like to explain the first reduced dimension $P_1$. Suppose also the correlations $corr(P_1, X_1)$, $corr(P_1, X_2)$ and $corr(P_1, X_3)$ are $-0.9, 0.3$ and $0.7$ respectively. Vector $r_1$ would written as
$$
    r_1 =
    \begin{bmatrix}
        -0.9 \\
        0.3  \\
        0.7
    \end{bmatrix},
$$

and vector $s_1$ as

$$
    s_1 =
    \begin{bmatrix}
        3 \\
        1 \\
        2
    \end{bmatrix}.
$$

Vector $s_i$ contains the information about the significance that the $i$-th component assigns to each of the original features. The position in the vector of the element of $s_i$ with the highest number indicates the feature that $s_i$ gives more importance to. In the example above, $X_1$ is the feature that $s_1$ gives more importance to because the first position of $s_1$ has the highest number, $3$. Subsequently, $X_3$ would be the second most important, and $X_2$ the least important. This allows us to compare and determine the similarity or dissimilarity of two components using the \textit{Spearman's rank correlation coefficient} of the absolute correlations. We do so by simply taking the correlation of vectors $s_i$ and $s_j$, there is no need for calculating the ranks, as that was done previously.

Following the previous example, suppose $s_1$ corresponds to the first projection of the reduced space discovered by $PCA$. We will refer to $s_1$ as $s_{PCA,1}$ from now on. Furthermore, let $s_{SLMVP,1} = [ 2, 1, 3 ]$ be the vector that corresponds to the first projection of the reduced space discovered by $SLMVP$. We compute the \textit{Spearman's rank correlation coefficient} by calculating $corr(s_{PCA,1}, s_{SLMVP,1})$ in the following manner.

$$
    corr(s_{PCA,1}, s_{SLMVP,1}) =
    corr \left(
    \begin{bmatrix}
            3 \\
            1 \\
            2
        \end{bmatrix},
    \begin{bmatrix}
            2 \\
            1 \\
            3
        \end{bmatrix}
    \right) = 0.5
$$

This way we can effectively calculate the similarities and dissimilarities of the first component calculated with the most prominent dimensionality reduction techniques and conclude that, for the dataset used, they are similar or dissimilar.

This can be further extended so that the we can compare all the components calculated with one dimensionality reduction technique with all the components calculated with another dimensionality reduction technique. In order to do that, we calculate a vector $s_1^*$ which corresponds to the average of $s_1, ..., s_p$ weighted by the variability that each component captures. This variability can be thought of as the eigenvalues corresponding the eigenvectors that form the projection matrix $V$. As I do not have access to the eigenvalues for some of the dimensionality reduction techniques, this is calculated proportionally to the variation of each component. E.g., $Var(s_1)/\sum_i Var(s_i)$ would be the weight corresponding to $s_1$.

\section{Choosing Features and Components}
% Big questions
There are two big open questions when applying dimensionality reduction in a machine learning pipeline: which features should one retain or discard?, and how many components should one utilize for subsequent modeling?
% Answers
The writer of this thesis gives a recommendation based on the explainability of components. As to the first question, only the features that show an absolute correlation that is higher than a threshold $\lambda$ should be chosen. Therefore the amount of features that are retained will depend on the choice of this threshold. The reader should be advised that this would work best with dimensionality reduction techniques that are supervised and not only try to capture the variability of the data, but also incorporate the relationship between dependent and independent variables.

The recommendation concerning the amount of components is to make that choice based on a heuristic visual analysis of a line plot of the eigenvalues of principal components, or of the cumulative explained variance, and look for the point where the curve starts to level off, this is sometimes known as the "elbow method".

\section{Experimental Setup}
This section details the selection and preparation of datasets, describes the dimensionality reduction techniques and machine learning algorithms utilized.

\subsection{Techniques and Models}
We want to explain dimensionality reduction techniques with the parameters that fit the data well, and we measure that by the accuracy that they yield when used in a machine learning pipeline. For that purpose, the algorithms are run in different settings and then machine learning models use the components for the classification task. These machine learning models are, in turn, also tested with different parameters. Table \ref{tab:models} shows the parameters tested for the dimensionality reduction techniques.

% Dimensionality Redcution Techniques Parameters Table
\begin{table}[!ht]
    \centering
    \begin{tabular}{lp{12cm}}
        \toprule
        Model                                                & Parameters                                                                                                                                 \\
        \midrule
        PCA \cite{pca}                                       & Default parameters.                                                                                                                        \\[2mm]
        \rowcolor{lightgray}SLMVP \cite{slmvp}               & Kernel: linear, polynomial of order 5, radial rbf with gamma values $0.1$ and $0.01$ for both the dependent and the independent variables. \\[2mm]
        LOL \cite{lol}                                       & Default parameters.                                                                                                                        \\[2mm]
        \rowcolor{lightgray}KPCA  \cite{kpca}                & linear, polynomial of order 5, radial rbf with gamma value $0.1$ and $\frac{1}{\text{\# rows}}$.                                           \\[2mm]
        LPP \cite{lpp}                                       & Number of neighbors to consider on the adjacency graph: $\lfloor\sqrt{min(\text{\# rows, \# columns})}\rfloor$.                            \\[2mm]
        \rowcolor{lightgray}                                 & Number of neighbors to generate the local linear approximation: $\lfloor\sqrt{min(\text{\# rows, \# columns})}\rfloor$.                    \\[2mm]
        \rowcolor{lightgray}\multirow{-2}{*}{LLE \cite{lle}} & Regularization parameter: $0.001$.                                                                                                         \\
        \bottomrule
    \end{tabular}
    \caption{Parameters tested for each of the dimensionality reduction techniques.}
    \label{tab:techniques}
\end{table}

Similarly, the machine learning classifiers have been configured on the grid search with the the parameters in Table \ref{tab:models}.


\begin{table}[!ht]
    \centering
    \begin{tabular}{lp{11cm}}
        \toprule
        Model                                                                   & Parameters                                      \\
        \midrule
        XGBoost \cite{xgboost}                                                  & Number of estimators: 5, 10, 20, 50, and 100.   \\[2mm]
        \rowcolor{lightgray} KNN \cite{knn}                                     & Number of neighbors: 3, 5, 10, 20, 50, and 100. \\[2mm]
                                                                                & Regularization parameter C: 0.1, 1, and 10.     \\
        \multirow{-2}{*}{SVM \cite{svm}}                                        & Kernel: linear, radial rbf and polynomial.      \\[2mm]
        \rowcolor{lightgray} Decision Tree                                      & Maximum depth: None, 5, 10, 20, and 50.         \\[2mm]
        Naive Bayes                                                             & Default parameters.                             \\[2mm]
        \rowcolor{lightgray}                                                    & Number of estimators: 50, 100, and 200.         \\
        \rowcolor{lightgray}\multirow{-2}{*}{Random Forest \cite{randomforest}} & Maximum depth: None, 5, 10, and 20              \\[2mm]
        LDA \cite{lda}                                                          & Solver: svd, lsqr, and eigen.                   \\[2mm]
        \rowcolor{lightgray}                                                    & Number of estimators: 50, 100, and 200.         \\
        \rowcolor{lightgray}\multirow{-2}{*}{AdaBoost \cite{adaboost}}          & Maximum depth: None, 5, 10, and 20.             \\[2mm]
        \bottomrule
    \end{tabular}
    \caption{Parameters tested for each of the machine learning models.}
    \label{tab:models}
\end{table}

% ML Libraries

\begin{table}[!ht]
    \begin{center}
        \begin{tabular}{llllr}
            \toprule
            Model         & Library                                                   \\
            \midrule
            XGBoost       & xgboost.XGBClassifier                                     \\
            KNN           & sklearn.neighbors.KNeighborsClassifier                    \\
            SVM           & sklearn.svm.SVC                                           \\
            Decision Tree & sklearn.tree.DecisionTreeClassifier                       \\
            Naive Bayes   & sklearn.naive\_bayes.GaussianNB                           \\
            Random Forest & sklearn.ensemble.RandomForestClassifier                   \\
            LDA           & sklearn.discriminant\_analysis.LinearDiscriminantAnalysis \\
            AdaBoost      & sklearn.ensemble.AdaBoostClassifier                       \\
            \bottomrule
        \end{tabular}
    \end{center}
    \caption{Python libraries containing the implementation of the machine learning algorithms utilized.}
    \label{tab:libraries-models}
\end{table}


% Dim Red Libraries
\begin{table}[!ht]
    \begin{center}
        \begin{tabular}{llllr}
            \toprule
            Model & Library                                 \\
            \midrule
            PCA   & sklearn.decomposition.PCA               \\
            KPCA  & sklearn.decomposition.KernelPCA         \\
            LOL   & lol.LOL                                 \\
            LPP   & lpproj.LocalityPreservingProjection     \\
            LLE   & sklearn.manifold.LocallyLinearEmbedding \\
            \bottomrule
        \end{tabular}
    \end{center}
    \caption{Python libraries containing the implementation of the dimensionality reduction techniques utilized.}
    \label{tab:libraries-techniques}
\end{table}



\subsection{Datasets}
This section introduces the datasets utilized in this master's thesis. The aim is to provide a comprehensive overview of the datasets, including their sources, characteristics, and any preprocessing steps applied.

\subsubsection{Artificial Datasets}\label{section:artificial-dataset}
% Single Label
Sklearn's make\_blobs was utilized to create a single-label dataset. It created a multiclass datasets by allocating each class one or more normally-distributed clusters of points. The dataset was of sample size 1000, 300 features and 20 classes, with a cluster standard deviation of 2. The high number of features and low sample size is perfect for the "large $p$, small $n$" problem that dimensionality reduction tries to address.
% Multilabel
Sklearn's make\_multilabel\_classification was utilized to create a multilabel dataset.

\subsubsection{FIFA}\label{section:fifa-dataset}
The FIFA dataset encompasses data from various editions of the popular FIFA video game series, which serves as a reliable source of player information. The dataset contains detailed attributes for each player, including playing positions, skill ratings (such as dribbling, shooting, and passing), and performance statistics (such as goals, assists, and appearances).

Furthermore, it includes the estimated market value of each player, which has often been used as the dependent variable in a regression or classification task.

\subsubsection{Our Database of Faces (ORL) Dataset}\label{section:orl-dataset}
This dataset \cite{orl_dataset} was created at the AT\&T Laboratories in Cambridge, UK, in the context of a face recognition project the laboratory was doing with the Speech, Vision and Robotics Group of the Cambridge University Engineering Department. It contains face images taken between April 1992 and April 1994, 10 images of 40 different subjects, a total of 400 images. It consists of grayscale images of 40 individuals, with 10 images per person. The dataset was collected under controlled conditions, with variations in facial expression, lighting conditions, and facial details.

Each image in the ORL dataset has a resolution of 92 pixels by 112 pixels. The images were captured under different poses, including variations in head rotation and tilt, providing a diverse set of facial orientations. The ORL dataset offers a realistic representation of face images encountered in real-world scenarios, making it a suitable choice for evaluating the performance of face recognition algorithms. The dataset has been extensively used for training and testing various dimensionality reduction techniques.

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{The-ORL-database-for-training-and-testing.png}
    \caption{Our Database of Faces (ORL) Dataset}
    \label{fig:orl_faces}
\end{figure}

\subsubsection{COIL2000}\label{section:coil-dataset}
This dataset used in the CoIL 2000 Challenge contains information on customers of an insurance company.\cite{coil} The data consists of 86 variables and includes product usage data and socio-demographic data. The data was supplied by the Dutch data mining company Sentient Machine Research and is based on a real world business problem. The training set contains over 5000 descriptions of customers, including the information of whether or not they have a caravan insurance policy. A test set contains 4000 customers of whom only the organizers know if they have a caravan insurance policy.

The dataset had class imbalance, as a large majority of customers have zero claims, only 348 customers had a claim. Sklearn's \textit{RandomOverSampler} was used to balance it.
%\cite{van2004bias}
